{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "78a341c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/Marta/.cache/kagglehub/datasets/trainingdatapro/cars-video-object-tracking/versions/3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"trainingdatapro/cars-video-object-tracking\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "DATASET_DIR = Path(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "c53758f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames: 301\n"
     ]
    }
   ],
   "source": [
    "IMAGE_DIR = DATASET_DIR / \"images\"\n",
    "\n",
    "assert IMAGE_DIR.exists(), f\"Missing {IMAGE_DIR}\"\n",
    "def sorted_images(folder: Path):\n",
    "    exts = (\".png\", \".jpg\", \".jpeg\", \".bmp\")\n",
    "    files = [p for p in folder.iterdir() if p.suffix.lower() in exts]\n",
    "    files.sort(key=lambda p: p.name)\n",
    "    return files\n",
    "\n",
    "IMAGE_FILES = sorted_images(IMAGE_DIR)\n",
    "\n",
    "print(\"Frames:\", len(IMAGE_FILES))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18095d20",
   "metadata": {},
   "source": [
    "## Detection\n",
    "\n",
    "This section extracts moving objects from video frames using background subtraction and morphological cleaning. We use the MOG2 algorithm to separate foreground (moving objects) from the static background. To remove noise, we apply morphological operations such as opening, closing, and dilation. We then use connected components with perspective awareness to extract individual objects from the mask. \n",
    "\n",
    "Because the mask is imperfect due to various factors like vehicle color, distance to camera, and lighting conditions, a single car sometimes appears divided into multiple disconnected blobs. That's why we cluster and group the blobs that are likely to be part of the same vehicle, taking perspective into account. Since blobs far from the camera that are close together could belong to different vehicles, but as they come closer to the camera the distance between vehicles increases, we use perspective-aware clustering to  merge fragments while avoiding over-merging distant vehicles.\n",
    "\n",
    "### Functions\n",
    "\n",
    "TODO: explain each function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "ba291fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_mask(mask: np.ndarray) -> np.ndarray:\n",
    "\n",
    "    # Remove shadow pixels from MOG2 \n",
    "    _, mask = cv2.threshold(mask, 200, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    k2 = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, k2, iterations=2)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def cluster_blobs_by_proximity(dets: list, distance_threshold: float =100.0) -> list:\n",
    "    \"\"\"\n",
    "    Merge blobs that are very close to each other (likely fragments of same vehicle).\n",
    "    Simplified: assumes cars are well-defined, uses fixed distance threshold.\n",
    "    \"\"\"\n",
    "    if len(dets) <= 1:\n",
    "        return dets\n",
    "    \n",
    "    n = len(dets)\n",
    "    clusters = list(range(n))\n",
    "    \n",
    "    def find_root(i):\n",
    "        if clusters[i] != i:\n",
    "            clusters[i] = find_root(clusters[i])\n",
    "        return clusters[i]\n",
    "    \n",
    "    def union(i, j):\n",
    "        root_i, root_j = find_root(i), find_root(j)\n",
    "        if root_i != root_j:\n",
    "            clusters[root_j] = root_i\n",
    "    \n",
    "    # Simple pairwise distance check\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            cx_i, cy_i = dets[i][\"centroid\"]\n",
    "            cx_j, cy_j = dets[j][\"centroid\"]\n",
    "            d = ((cx_j - cx_i)**2 + (cy_j - cy_i)**2) ** 0.5\n",
    "            \n",
    "            if d < distance_threshold:\n",
    "                union(i, j)\n",
    "    \n",
    "    # Merge clusters\n",
    "    cluster_map = {}\n",
    "    for i in range(n):\n",
    "        root = find_root(i)\n",
    "        if root not in cluster_map:\n",
    "            cluster_map[root] = []\n",
    "        cluster_map[root].append(i)\n",
    "    \n",
    "    merged = []\n",
    "    for cluster_indices in cluster_map.values():\n",
    "        cluster_dets = [dets[i] for i in cluster_indices]\n",
    "        if len(cluster_dets) == 1:\n",
    "            merged.append(cluster_dets[0])\n",
    "        else:\n",
    "            total_area = sum(d[\"area\"] for d in cluster_dets)\n",
    "            merged_cx = sum(d[\"centroid\"][0] * d[\"area\"] for d in cluster_dets) / total_area\n",
    "            merged_cy = sum(d[\"centroid\"][1] * d[\"area\"] for d in cluster_dets) / total_area\n",
    "            \n",
    "            all_xs = [d[\"bbox\"][0] for d in cluster_dets] + [d[\"bbox\"][0] + d[\"bbox\"][2] for d in cluster_dets]\n",
    "            all_ys = [d[\"bbox\"][1] for d in cluster_dets] + [d[\"bbox\"][1] + d[\"bbox\"][3] for d in cluster_dets]\n",
    "            x_min, x_max = min(all_xs), max(all_xs)\n",
    "            y_min, y_max = min(all_ys), max(all_ys)\n",
    "            \n",
    "            merged.append({\n",
    "                \"centroid\": (int(merged_cx), int(merged_cy)),\n",
    "                \"bbox\": (x_min, y_min, x_max - x_min, y_max - y_min),\n",
    "                \"area\": total_area\n",
    "            })\n",
    "    \n",
    "    return merged\n",
    "\n",
    "def allowed_area_range(y, img_h):\n",
    "    \"\"\"\n",
    "    Perspective model with realistic lower bounds for distant vehicles.\n",
    "    Uses conservative minimum (300 px²) to catch small distant cars,\n",
    "    while maintaining quadratic growth for closer vehicles.\n",
    "    \"\"\"\n",
    "    t = y / img_h   # 0 top → 1 bottom\n",
    "\n",
    "    # FIXED: Lower base to catch distant cars (300-800 px²)\n",
    "    # Original empirical model was too restrictive (base=2000)\n",
    "    min_area = 300 + 25000 * t * t  # Allows 300 px² at top, ~25k at bottom\n",
    "    max_area = 800 + 110000 * t * t  # Allows 800 px² at top, ~110k at bottom\n",
    "\n",
    "    return min_area, max_area\n",
    "\n",
    "def detect_blobs(mask: np.ndarray, roi_mask=None, min_area=1500):\n",
    "\n",
    "    if roi_mask is not None:\n",
    "        mask = cv2.bitwise_and(mask, roi_mask)\n",
    "\n",
    "    H, W = mask.shape[:2]\n",
    "\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    dets = []\n",
    "    for c in contours:\n",
    "        area = cv2.contourArea(c)\n",
    "        if area <= 0:\n",
    "            continue\n",
    "\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "\n",
    "        # perspective filtering: area must be plausible for its vertical position (y)\n",
    "        minA, maxA = allowed_area_range(y, H)  # Use y (top) instead of y+h (bottom)\n",
    "        if not (minA <= area <= maxA):\n",
    "            continue\n",
    "\n",
    "        if area < 150:\n",
    "            continue  # Lower minimum area threshold for small cars\n",
    "\n",
    "        M = cv2.moments(c)\n",
    "        if M[\"m00\"] == 0:\n",
    "            continue\n",
    "\n",
    "        cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "        cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "\n",
    "        dets.append({\"centroid\": (cx, cy), \"bbox\": (x, y, w, h), \"area\": area})\n",
    "\n",
    "    # Cluster and merge nearby fragmented blobs (perspective-aware)\n",
    "    dets = cluster_blobs_by_proximity(dets, distance_threshold=40.0)\n",
    "    return dets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8df942",
   "metadata": {},
   "source": [
    "## Tracking\n",
    "\n",
    "Next, we need to track each detected car. We chose the Kalman filter because it is fast, supports multiple object tracking, and predicts the probable next position of each car based on its own motion model. Different cars move at different speeds, and the Kalman filter learns and adapts to these individual motion patterns.\n",
    "\n",
    "The Kalman filter maintains a state vector [x, y, vx, vy] representing position and velocity. The transition matrix defines how the state evolves between frames (it predicts where the car will be based on its current velocity). The measurement matrix maps the detected position (which we observe) to the state space. The process noise covariance controls how much we trust the motion model (lower values = trust motion more), while the measurement noise covariance controls how much we trust the detections (lower values = trust detections more). By balancing these, the filter smooths noisy detections while allowing the object to change speed.\n",
    "\n",
    "Each detection is attached to a Track object, which aims to maintain a persistent identity throughout the video. Every track has a unique ID and its own Kalman filter. The predict function estimates where the vehicle should be in the current frame using the motion model, before we see any new detections. The update function corrects the prediction using the actual detected position, allowing the filter to learn and adjust if the vehicle's motion changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "8718bbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_kalman(dt: float = 1.0,\n",
    "                process_var: float = 50.0,\n",
    "                meas_var: float = 25.0) -> cv2.KalmanFilter:\n",
    "    \"\"\"\n",
    "    State: [x, y, vx, vy]^T\n",
    "    Measurement: [x, y]^T\n",
    "    \"\"\"\n",
    "    kf = cv2.KalmanFilter(4, 2)\n",
    "\n",
    "    kf.transitionMatrix = np.array([\n",
    "        [1, 0, dt, 0 ],\n",
    "        [0, 1, 0 , dt],\n",
    "        [0, 0, 1 , 0 ],\n",
    "        [0, 0, 0 , 1 ],\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    kf.measurementMatrix = np.array([\n",
    "        [1, 0, 0, 0],\n",
    "        [0, 1, 0, 0],\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    # These are the two main tuning knobs.\n",
    "    kf.processNoiseCov = np.eye(4, dtype=np.float32) * process_var\n",
    "    kf.measurementNoiseCov = np.eye(2, dtype=np.float32) * meas_var\n",
    "\n",
    "    # Start uncertain so it can adapt quickly.\n",
    "    kf.errorCovPost = np.eye(4, dtype=np.float32) * 500.0\n",
    "    kf.statePost = np.zeros((4, 1), dtype=np.float32)\n",
    "    return kf\n",
    "\n",
    "def color_from_id(track_id: int) -> tuple[int, int, int]:\n",
    "\n",
    "    rng = np.random.default_rng(track_id)  # stable seed per id\n",
    "    # Keep colors away from extremes: [40..220]\n",
    "    c = rng.integers(40, 220, size=3, dtype=np.int32)\n",
    "    return (int(c[0]), int(c[1]), int(c[2]))  # B, G, R\n",
    "\n",
    "class Track:\n",
    "    \"\"\"\n",
    "    One vehicle hypothesis + identity.\n",
    "    We keep:\n",
    "      - Kalman filter\n",
    "      - hits: how many times we matched a detection (confidence)\n",
    "      - missed: how many consecutive frames we failed to match (death timer)\n",
    "      - confirmed: whether track has been stable for MIN_HITS frames\n",
    "    \"\"\"\n",
    "    def __init__(self, track_id: int, init_xy: tuple[int,int], init_bbox, dt=1.0,\n",
    "                 process_var=50.0, meas_var=25.0):\n",
    "        self.id = track_id\n",
    "        self.kf = init_kalman(dt=dt, process_var=process_var, meas_var=meas_var)\n",
    "        self.color = color_from_id(track_id)\n",
    "\n",
    "        x, y = init_xy\n",
    "        self.kf.statePost = np.array([[x], [y], [0], [0]], dtype=np.float32)\n",
    "\n",
    "        self.hits = 1\n",
    "        self.missed = 0\n",
    "        self.confirmed = False  # Only show after MIN_HITS consecutive matches\n",
    "        self.bbox = init_bbox\n",
    "        self.history = [init_xy]\n",
    "        self.consecutive_hits = 1  # Count consecutive frames with detections\n",
    "\n",
    "    def predict(self) -> tuple[float,float]:\n",
    "        \"\"\"\n",
    "        Predict where the vehicle should be in the current frame (before seeing detections).\n",
    "        Important: use the predicted output from predict(), not statePost.\n",
    "        \"\"\"\n",
    "        pred = self.kf.predict()\n",
    "        return float(pred[0]), float(pred[1])\n",
    "\n",
    "    def update(self, xy: tuple[int,int], bbox):\n",
    "        \"\"\"\n",
    "        Correct the predicted state using the detection measurement.\n",
    "        \"\"\"\n",
    "        cx, cy = xy\n",
    "        z = np.array([[cx], [cy]], dtype=np.float32)\n",
    "        self.kf.correct(z)\n",
    "        self.hits += 1\n",
    "        self.consecutive_hits += 1  # Increment consecutive hits\n",
    "        self.missed = 0\n",
    "        self.bbox = bbox\n",
    "        self.history.append(xy)\n",
    "\n",
    "    def mark_missed(self):\n",
    "        \"\"\"\n",
    "        No detection matched this track this frame.\n",
    "        \"\"\"\n",
    "        self.missed += 1\n",
    "        self.consecutive_hits = 0  # Reset consecutive hits\n",
    "\n",
    "    def is_confirmed(self, min_hits: int = 3) -> bool:\n",
    "        \"\"\"Check if track is stable and should be displayed.\"\"\"\n",
    "        return self.consecutive_hits >= min_hits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "dcc88691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adaptive association with measurement validation\n",
    "\n",
    "def match_distance_for_bbox(bbox):\n",
    "    \"\"\"\n",
    "    Larger objects can move more pixels between frames.\n",
    "    bbox height ≈ distance to camera (perspective effect).\n",
    "    Adaptive gate: closer objects (larger) can move more.\n",
    "    \"\"\"\n",
    "    _, _, w, h = bbox\n",
    "    return 20 + 0.8 * h   # slightly more permissive than before\n",
    "\n",
    "def euclidean_distance(track, detection):\n",
    "    \"\"\"\n",
    "    Compute Euclidean distance between track prediction and detection.\n",
    "    \"\"\"\n",
    "    pred_state = track.kf.statePost\n",
    "    pred_x, pred_y = float(pred_state[0]), float(pred_state[1])\n",
    "    det_x, det_y = detection[\"centroid\"]\n",
    "    d = ((pred_x - det_x)**2 + (pred_y - det_y)**2) ** 0.5\n",
    "    return d\n",
    "\n",
    "\n",
    "def associate_detections_to_tracks(dets, tracks, min_confirmed_hits=3):\n",
    "    \"\"\"\n",
    "    For each track, pick the nearest detection,\n",
    "    but only if it lies within a size-dependent distance.\n",
    "    \n",
    "    Prioritize confirmed tracks to prevent ghost IDs from stealing detections.\n",
    "    Validate measurements: reject detections that would cause large corrections\n",
    "    to avoid mask artifacts corrupting track state.\n",
    "    \n",
    "    OPTIMIZED: Use sets instead of list.remove() to avoid O(n) operations.\n",
    "    \"\"\"\n",
    "\n",
    "    matches = []\n",
    "    unmatched_dets = set(range(len(dets)))\n",
    "    unmatched_tracks = set(range(len(tracks)))\n",
    "\n",
    "    # First pass: match CONFIRMED tracks (higher priority)\n",
    "    for ti, t in enumerate(tracks):\n",
    "        if not t.is_confirmed(min_confirmed_hits) or not unmatched_dets:\n",
    "            continue\n",
    "\n",
    "        tx, ty = t.history[-1]\n",
    "        best_di = None\n",
    "        best_d = float(\"inf\")\n",
    "\n",
    "        for di in unmatched_dets:\n",
    "            cx, cy = dets[di][\"centroid\"]\n",
    "            d = ((tx - cx)**2 + (ty - cy)**2) ** 0.5\n",
    "            allowed = match_distance_for_bbox(dets[di][\"bbox\"])\n",
    "\n",
    "            if d < allowed and d < best_d:\n",
    "                best_d = d\n",
    "                best_di = di\n",
    "\n",
    "        if best_di is not None:\n",
    "            # Additional validation: check that this detection is plausible\n",
    "            # using Kalman filter's predicted state (not just history)\n",
    "            euclid_d = euclidean_distance(t, dets[best_di])\n",
    "            pred_allowed = match_distance_for_bbox(dets[best_di][\"bbox\"]) * 1.2  # 20% relaxation for KF\n",
    "            if euclid_d < pred_allowed:\n",
    "                matches.append((ti, best_di))\n",
    "                unmatched_dets.discard(best_di)\n",
    "                unmatched_tracks.discard(ti)\n",
    "\n",
    "    # Second pass: match UNCONFIRMED tracks (lower priority, stricter matching)\n",
    "    for ti, t in enumerate(tracks):\n",
    "        if t.is_confirmed(min_confirmed_hits) or ti not in unmatched_tracks or not unmatched_dets:\n",
    "            continue\n",
    "\n",
    "        tx, ty = t.history[-1]\n",
    "        best_di = None\n",
    "        best_d = float(\"inf\")\n",
    "\n",
    "        for di in unmatched_dets:\n",
    "            cx, cy = dets[di][\"centroid\"]\n",
    "            d = ((tx - cx)**2 + (ty - cy)**2) ** 0.5\n",
    "            # Stricter gate for unconfirmed tracks (avoid spurious matches)\n",
    "            allowed = match_distance_for_bbox(dets[di][\"bbox\"]) * 0.7\n",
    "\n",
    "            if d < allowed and d < best_d:\n",
    "                best_d = d\n",
    "                best_di = di\n",
    "\n",
    "        if best_di is not None:\n",
    "            matches.append((ti, best_di))\n",
    "            unmatched_dets.discard(best_di)\n",
    "            unmatched_tracks.discard(ti)\n",
    "\n",
    "    return matches, list(unmatched_tracks), list(unmatched_dets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c6255e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording video to: vehicle_tracking_debug.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sz/4qbyglh16n3gzj4l7_jkj3rh0000gn/T/ipykernel_8084/800847209.py:69: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return float(pred[0]), float(pred[1])\n",
      "/var/folders/sz/4qbyglh16n3gzj4l7_jkj3rh0000gn/T/ipykernel_8084/1739340089.py:17: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  pred_x, pred_y = float(pred_state[0]), float(pred_state[1])\n",
      "/var/folders/sz/4qbyglh16n3gzj4l7_jkj3rh0000gn/T/ipykernel_8084/1739340089.py:17: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  pred_x, pred_y = float(pred_state[0]), float(pred_state[1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: vehicle_tracking_debug.mp4\n"
     ]
    }
   ],
   "source": [
    "# main multi-object tracking loop with ID fragmentation fixes\n",
    "\n",
    "SAVE_VIDEO = True\n",
    "VIDEO_NAME = \"vehicle_tracking_debug.mp4\"\n",
    "VIDEO_FPS = 20\n",
    "video_writer = None\n",
    "\n",
    "DT = 1.0\n",
    "MAX_DIST = 50.0      # association gate (pixels)\n",
    "MAX_AGE  = 15        # INCREASED: frames allowed to miss before deleting (allows recovery from mask gaps)\n",
    "MIN_HITS = 3         # show track after this many consecutive matches\n",
    "MIN_AREA = 1200      # blob area threshold (slightly lower to catch fragments)\n",
    "\n",
    "PROCESS_VAR = 1e-2   # OpenCV KF units\n",
    "MEAS_VAR    = 1e-1\n",
    "\n",
    "# Blob merging parameters\n",
    "MERGE_DISTANCE = 50.0  # pixels: merge blobs closer than this\n",
    "SIZE_RATIO = 0.6       # allow merging if area ratio is between 0.6 and 1/0.6\n",
    "\n",
    "# Background subtractor (learns background online)\n",
    "bg = cv2.createBackgroundSubtractorMOG2(history=400, varThreshold=16, detectShadows=True)\n",
    "\n",
    "# ROI mask: set to None or supply a binary mask (255=keep, 0=ignore)\n",
    "roi_mask = None\n",
    "\n",
    "tracks = []\n",
    "next_id = 1\n",
    "\n",
    "SHOW_EVERY = 1   # increase if too slow\n",
    "SCALE = 0.6      # scale for display screen\n",
    "\n",
    "def draw_bbox(img, bbox, label, color, thickness=2):\n",
    "    x, y, w, h = bbox\n",
    "    x2, y2 = x + w, y + h\n",
    "\n",
    "    # optional \"shadow\" outline for contrast\n",
    "    cv2.rectangle(img, (x, y), (x2, y2), (0, 0, 0), thickness + 2, lineType=cv2.LINE_AA)\n",
    "    cv2.rectangle(img, (x, y), (x2, y2), color, thickness, lineType=cv2.LINE_AA)\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.55\n",
    "    text_thickness = 2\n",
    "    (tw, th), baseline = cv2.getTextSize(label, font, font_scale, text_thickness)\n",
    "\n",
    "    y_text_top = y - (th + baseline + 6)\n",
    "    if y_text_top < 0:\n",
    "        y_text_top = y + 2\n",
    "\n",
    "    x_text = x\n",
    "    y_text = y_text_top + th + 3\n",
    "\n",
    "    cv2.rectangle(img, (x_text, y_text_top), (x_text + tw + 8, y_text_top + th + baseline + 6),\n",
    "                  (0, 0, 0), -1, lineType=cv2.LINE_AA)\n",
    "    cv2.rectangle(img, (x_text + 1, y_text_top + 1), (x_text + tw + 7, y_text_top + th + baseline + 5),\n",
    "                  color, -1, lineType=cv2.LINE_AA)\n",
    "\n",
    "    cv2.putText(img, label, (x_text + 4, y_text),\n",
    "                font, font_scale, (0, 0, 0), text_thickness, lineType=cv2.LINE_AA)\n",
    "\n",
    "\n",
    "# Main loop \n",
    "# Each car (track) gets its own Kalman filter, which maintains a state vector for position and velocity.\n",
    "#For every frame, the Kalman filter predicts where the car should be (the predicted position).\n",
    "# When new detections (blobs) are found, the code tries to match them to existing tracks using both the last known position and the Kalman filter's prediction.\n",
    "#If a detection matches a track, the Kalman filter updates (corrects) its state using the detected position.\n",
    "\n",
    "\n",
    "for i, img_path in enumerate(IMAGE_FILES):\n",
    "    frame = cv2.imread(str(img_path))\n",
    "    if frame is None:\n",
    "        continue\n",
    "\n",
    "    H, W = frame.shape[:2]\n",
    "\n",
    "    # 1) Foreground mask (motion)\n",
    "    fg = bg.apply(frame)\n",
    "\n",
    "    # 2) Clean the mask\n",
    "    fg_clean = clean_mask(fg)\n",
    "\n",
    "    # 3) Detections = blobs (with merging of fragments)\n",
    "    dets = detect_blobs(fg_clean, roi_mask=roi_mask, min_area=MIN_AREA)\n",
    "\n",
    "    # 4) Predict all tracks (KF motion model)\n",
    "    pred_xy = [t.predict() for t in tracks]\n",
    "\n",
    "    # 5) Associate with two-pass approach (confirmed tracks first)\n",
    "    matches, unmatched_tracks, unmatched_dets = associate_detections_to_tracks(\n",
    "        dets, tracks, min_confirmed_hits=MIN_HITS\n",
    "    )\n",
    "\n",
    "    # 6) Update matched tracks (KF correct step)\n",
    "    for ti, di in matches:\n",
    "        cx, cy = dets[di][\"centroid\"]\n",
    "        bbox = dets[di][\"bbox\"]\n",
    "        tracks[ti].update((cx, cy), bbox)\n",
    "\n",
    "    # 7) Mark unmatched tracks\n",
    "    for ti in unmatched_tracks:\n",
    "        tracks[ti].mark_missed()\n",
    "\n",
    "    # 8) Create new tracks for unmatched detections\n",
    "    for di in unmatched_dets:\n",
    "        cx, cy = dets[di][\"centroid\"]\n",
    "        bbox = dets[di][\"bbox\"]\n",
    "\n",
    "        # duplicate suppression: don't create new ID if near any existing track \n",
    "        duplicate = False\n",
    "        for t in tracks:\n",
    "            tx, ty = t.history[-1]\n",
    "            dist = ((tx - cx)**2 + (ty - cy)**2) ** 0.5\n",
    "            threshold = 40 if not t.is_confirmed(MIN_HITS) else 50\n",
    "            if dist < threshold:\n",
    "                duplicate = True\n",
    "                break\n",
    "        if duplicate:\n",
    "            continue\n",
    "\n",
    "        # Only create track for reasonably sized detections (avoid noise)\n",
    "        if dets[di][\"area\"] < MIN_AREA * 0.8:\n",
    "            continue\n",
    "\n",
    "        # Allow new track creation anywhere in the frame\n",
    "\n",
    "        tracks.append(Track(\n",
    "            track_id=next_id,\n",
    "            init_xy=(cx, cy),\n",
    "            init_bbox=bbox,\n",
    "            dt=DT,\n",
    "            process_var=PROCESS_VAR,\n",
    "            meas_var=MEAS_VAR\n",
    "        ))\n",
    "        next_id += 1\n",
    "\n",
    "    # 9) Delete only truly stale tracks (high MAX_AGE tolerance)\n",
    "    tracks = [t for t in tracks if t.missed <= MAX_AGE]\n",
    "\n",
    "    # 10) Visualize + record (mask top, tracking bottom)\n",
    "    if i % SHOW_EVERY == 0:\n",
    "        vis = frame.copy()\n",
    "\n",
    "        # Draw only CONFIRMED tracks (prevents ID flicker)\n",
    "        for t in tracks:\n",
    "            if not t.is_confirmed(MIN_HITS):\n",
    "                continue\n",
    "            if t.missed > 1:\n",
    "                continue\n",
    "            label = f\"vehicle_{t.id}\"\n",
    "            draw_bbox(vis, t.bbox, label, t.color, thickness=2)\n",
    "\n",
    "        mask_vis = cv2.cvtColor(fg_clean, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        vis_small  = cv2.resize(vis, (int(W * SCALE), int(H * SCALE)))\n",
    "        mask_small = cv2.resize(mask_vis, (int(W * SCALE), int(H * SCALE)))\n",
    "\n",
    "        stacked = np.vstack([mask_small, vis_small])\n",
    "\n",
    "        # init writer once\n",
    "        if SAVE_VIDEO and video_writer is None:\n",
    "            hh, ww = stacked.shape[:2]\n",
    "            fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "            video_writer = cv2.VideoWriter(VIDEO_NAME, fourcc, VIDEO_FPS, (ww, hh))\n",
    "            print(\"Recording video to:\", VIDEO_NAME)\n",
    "\n",
    "        if SAVE_VIDEO and video_writer is not None:\n",
    "            video_writer.write(stacked)\n",
    "\n",
    "        cv2.imshow(\"vehicle tracking\", stacked)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Cleanup\n",
    "if video_writer is not None:\n",
    "    video_writer.release()\n",
    "    print(\"Saved:\", VIDEO_NAME)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sub11761",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
