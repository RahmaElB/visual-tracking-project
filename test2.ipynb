{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "78a341c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/Marta/.cache/kagglehub/datasets/trainingdatapro/cars-video-object-tracking/versions/3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"trainingdatapro/cars-video-object-tracking\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "DATASET_DIR = Path(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "c53758f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames: 301\n"
     ]
    }
   ],
   "source": [
    "IMAGE_DIR = DATASET_DIR / \"images\"\n",
    "\n",
    "assert IMAGE_DIR.exists(), f\"Missing {IMAGE_DIR}\"\n",
    "def sorted_images(folder: Path):\n",
    "    exts = (\".png\", \".jpg\", \".jpeg\", \".bmp\")\n",
    "    files = [p for p in folder.iterdir() if p.suffix.lower() in exts]\n",
    "    files.sort(key=lambda p: p.name)\n",
    "    return files\n",
    "\n",
    "IMAGE_FILES = sorted_images(IMAGE_DIR)\n",
    "\n",
    "print(\"Frames:\", len(IMAGE_FILES))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d81c0a",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "TODO: Introduce problem \n",
    "\n",
    "how are we going to solve it\n",
    "\n",
    "why background substraction\n",
    "why kalman filter\n",
    "\n",
    "brief "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18095d20",
   "metadata": {},
   "source": [
    "## Detection\n",
    "\n",
    "This section extracts moving objects from video frames using background subtraction and morphological cleaning. We use the MOG2 algorithm to separate foreground (moving objects) from the static background. To remove noise, we apply morphological operations such as opening, closing, and dilation. We then use connected components to extract individual objects from the mask.\n",
    "\n",
    "Because the mask is imperfect due to various factors like vehicle color, distance to camera, and lighting conditions, a single car sometimes appears divided into multiple disconnected blobs. That's why we cluster and group the blobs that are likely to be part of the same vehicle, using proximity-based clustering to merge fragments while avoiding over-merging distant vehicles.\n",
    "\n",
    "### Functions\n",
    "\n",
    "Each function is explained in its code comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "ba291fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_mask(mask: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" \n",
    "     First we apply opening to remove some noise, and then closing to fill small gaps.\n",
    "    We tested different configurations and this combination gave the most stable results.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _, mask = cv2.threshold(mask, 200, 255, cv2.THRESH_BINARY)\n",
    "    k_open = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    k_close = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2)) # note: this is a small value but works better for us than bigger values\n",
    "\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, k_open, iterations=2)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, k_close, iterations=2)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def cluster_blobs_by_proximity(dets: list, distance_threshold: float =100.0) -> list:\n",
    "    \"\"\"\n",
    "    Cluster detections that are close together and merge them into a single detection\n",
    "    TODO: explanation context for markdown\n",
    "        Why this is needed:\n",
    "    Background subtraction often produces multiple small blobs for a single car\n",
    "    (broken mask, gaps, reflections). If we keep them separated, the tracker may\n",
    "    create duplicated IDs or jittery centroids.\n",
    "\n",
    "    Clustering policy:\n",
    "    - We group detections based on the distance between their centroids.\n",
    "    - If two detections are closer than distance_threshold, we connect them.\n",
    "    - The clustering is transitive (A close to B and B close to C -> all become one cluster).\n",
    "      This is implemented with a Union-Find (Disjoint Set) structure.\n",
    "\n",
    "    How we merge a cluster:\n",
    "    - Area: sum of areas (represents total blob size).\n",
    "    - Centroid: area-weighted average (bigger fragments contribute more).\n",
    "    - Bounding box: the tight bounding box that contains all member boxes.\n",
    "\n",
    "    Practical note / trade-off:\n",
    "    A larger distance_threshold fixes more fragmentation, but it can also merge\n",
    "    two different cars when they are very close (traffic or adjacent lanes).\n",
    "    So this parameter controls a balance between \"repairing broken blobs\" and\n",
    "    \"accidentally merging nearby vehicles\".\n",
    "    \"\"\"\n",
    "    if len(dets) <= 1:\n",
    "        return dets\n",
    "    n = len(dets)\n",
    "    clusters = list(range(n))\n",
    "    def find_root(i):\n",
    "        if clusters[i] != i:\n",
    "            clusters[i] = find_root(clusters[i])\n",
    "        return clusters[i]\n",
    "    def union(i, j):\n",
    "        root_i, root_j = find_root(i), find_root(j)\n",
    "        if root_i != root_j:\n",
    "            clusters[root_j] = root_i\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            cx_i, cy_i = dets[i][\"centroid\"]\n",
    "            cx_j, cy_j = dets[j][\"centroid\"]\n",
    "            d = ((cx_j - cx_i)**2 + (cy_j - cy_i)**2) ** 0.5\n",
    "            if d < distance_threshold:\n",
    "                union(i, j)\n",
    "    # Merge clusters\n",
    "    cluster_map = {}\n",
    "    for i in range(n):\n",
    "        root = find_root(i)\n",
    "        if root not in cluster_map:\n",
    "            cluster_map[root] = []\n",
    "        cluster_map[root].append(i)\n",
    "    merged = []\n",
    "    for cluster_indices in cluster_map.values():\n",
    "        cluster_dets = [dets[i] for i in cluster_indices]\n",
    "        if len(cluster_dets) == 1:\n",
    "            merged.append(cluster_dets[0])\n",
    "        else:\n",
    "            total_area = sum(d[\"area\"] for d in cluster_dets)\n",
    "            merged_cx = sum(d[\"centroid\"][0] * d[\"area\"] for d in cluster_dets) / total_area\n",
    "            merged_cy = sum(d[\"centroid\"][1] * d[\"area\"] for d in cluster_dets) / total_area\n",
    "            all_xs = [d[\"bbox\"][0] for d in cluster_dets] + [d[\"bbox\"][0] + d[\"bbox\"][2] for d in cluster_dets]\n",
    "            all_ys = [d[\"bbox\"][1] for d in cluster_dets] + [d[\"bbox\"][1] + d[\"bbox\"][3] for d in cluster_dets]\n",
    "            x_min, x_max = min(all_xs), max(all_xs)\n",
    "            y_min, y_max = min(all_ys), max(all_ys)\n",
    "            merged.append({\n",
    "                \"centroid\": (int(merged_cx), int(merged_cy)),\n",
    "                \"bbox\": (x_min, y_min, x_max - x_min, y_max - y_min),\n",
    "                \"area\": total_area\n",
    "            })\n",
    "    return merged\n",
    "\n",
    "def allowed_area_range(y_bottom, img_h):\n",
    "    \"\"\"\n",
    "    TODO: make this expalanation shorter here, its just for context when explaining in the markdown (this is an important part of the detector)\n",
    "    Vehicles appear larger when they are closer to the bottom of the image and\n",
    "    smaller when they are far away. To model this, we analysed the bounding box\n",
    "    annotations of the original video and studied how their pixel area grows\n",
    "    depending on their vertical position.\n",
    "\n",
    "    From this analysis we obtained two curves that estimate the expected minimum\n",
    "    and maximum area of a vehicle at each height of the image.\n",
    "\n",
    "    However, real detections from background subtraction are imperfect\n",
    "    (fragmented blobs, merged cars, lighting effects), and the raw formulas were\n",
    "    too strict and rejected valid vehicles. To make the detector more tolerant,\n",
    "    we apply scaling factors (kmin and kmax) that widen the allowed range so\n",
    "    slightly smaller or larger blobs are still accepted.\n",
    "   \n",
    "    \"\"\"\n",
    "    t = y_bottom / img_h\n",
    "    kmin = 0.3  # Adjusted to allow more variance, bigger c\n",
    "    kmax = 1.3\n",
    "    min_area = (2000.00 + 44749.12 * t * t) * kmin\n",
    "    max_area = (2000.00 + 108157.55 * t * t) * kmax\n",
    "    return min_area, max_area\n",
    "\n",
    "def detect_blobs(mask: np.ndarray):\n",
    "    H, W = mask.shape[:2]\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    dets = []\n",
    "    for c in contours:\n",
    "        area = cv2.contourArea(c)\n",
    "        if area <= 0:\n",
    "            continue\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "        y_bottom = y + h  # Use bottom of bbox for perspective model\n",
    "        minA, maxA = allowed_area_range(y_bottom, H)\n",
    "        if area <  minA or area > maxA:\n",
    "            continue\n",
    "        M = cv2.moments(c)\n",
    "        if M[\"m00\"] == 0:\n",
    "            continue\n",
    "        cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "        cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "        dets.append({\"centroid\": (cx, cy), \"bbox\": (x, y, w, h), \"area\": area})\n",
    "    # Cluster and merge nearby fragmented blobs\n",
    "    dets = cluster_blobs_by_proximity(dets, distance_threshold=40.0)\n",
    "    return dets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8df942",
   "metadata": {},
   "source": [
    "## Tracking\n",
    "\n",
    "Next, we need to track each detected car. We chose the Kalman filter because it is fast, supports multiple object tracking, and predicts the probable next position of each car based on its own motion model. Different cars move at different speeds, and the Kalman filter learns and adapts to these individual motion patterns.\n",
    "\n",
    "The Kalman filter maintains a state vector [x, y, vx, vy] representing position and velocity. The transition matrix defines how the state evolves between frames (it predicts where the car will be based on its current velocity). The measurement matrix maps the detected position (which we observe) to the state space. The process noise covariance controls how much we trust the motion model (lower values = trust motion more), while the measurement noise covariance controls how much we trust the detections (lower values = trust detections more). By balancing these, the filter smooths noisy detections while allowing the object to change speed.\n",
    "\n",
    "Each detection is attached to a Track object, which aims to maintain a persistent identity throughout the video. Every track has a unique ID and its own Kalman filter. The predict function estimates where the vehicle should be in the current frame using the motion model, before we see any new detections. The update function corrects the prediction using the actual detected position, allowing the filter to learn and adjust if the vehicle's motion changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "8718bbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_kalman(dt: float = 1.0, process_var=None, meas_var=None) -> cv2.KalmanFilter:\n",
    "    \"\"\"\n",
    "    State: [x, y, vx, vy]^T\n",
    "    Measurement: [x, y]^T\n",
    "    process_var, meas_var: empirical, in pixel units.\n",
    "    \"\"\"\n",
    "    kf = cv2.KalmanFilter(4, 2)\n",
    "\n",
    "    kf.transitionMatrix = np.array([\n",
    "        [1, 0, dt, 0 ],\n",
    "        [0, 1, 0 , dt],\n",
    "        [0, 0, 1 , 0 ],\n",
    "        [0, 0, 0 , 1 ],\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    kf.measurementMatrix = np.array([\n",
    "        [1, 0, 0, 0],\n",
    "        [0, 1, 0, 0],\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    # These are the two main tuning knobs.\n",
    "    kf.processNoiseCov = np.eye(4, dtype=np.float32) * process_var\n",
    "    kf.measurementNoiseCov = np.eye(2, dtype=np.float32) * meas_var\n",
    "\n",
    "    # Start uncertain so it can adapt quickly.\n",
    "    kf.errorCovPost = np.eye(4, dtype=np.float32) * 500.0\n",
    "    kf.statePost = np.zeros((4, 1), dtype=np.float32)\n",
    "    return kf\n",
    "\n",
    "def color_from_id(track_id: int) -> tuple[int, int, int]:\n",
    "\n",
    "    rng = np.random.default_rng(track_id)  # stable seed per id\n",
    "    # Keep colors away from extremes: [40..220]\n",
    "    c = rng.integers(40, 220, size=3, dtype=np.int32)\n",
    "    return (int(c[0]), int(c[1]), int(c[2]))  # B, G, R\n",
    "\n",
    "class Track:\n",
    "    \"\"\"\n",
    "    One vehicle hypothesis + identity.\n",
    "    We keep:\n",
    "      - Kalman filter\n",
    "      - hits: how many times we matched a detection (confidence)\n",
    "      - missed: how many consecutive frames we failed to match (death timer)\n",
    "      - confirmed: whether track has been stable for MIN_HITS frames\n",
    "    \"\"\"\n",
    "    def __init__(self, track_id: int, init_xy: tuple[int,int], init_bbox, dt=1.0, process_var=None, meas_var=None):\n",
    "        self.id = track_id\n",
    "        self.kf = init_kalman(dt=dt, process_var=process_var, meas_var=meas_var)\n",
    "        self.color = color_from_id(track_id)\n",
    "\n",
    "        x, y = init_xy\n",
    "        self.kf.statePost = np.array([[x], [y], [0], [0]], dtype=np.float32)\n",
    "\n",
    "        self.hits = 1\n",
    "        self.missed = 0\n",
    "        self.bbox = init_bbox\n",
    "        self.history = [init_xy]\n",
    "        self.consecutive_hits = 1  # Count consecutive frames with detections\n",
    "        self.last_pred = (x, y)  # Initialize with first position\n",
    "\n",
    "    def predict(self) -> tuple[float,float]:\n",
    "        \"\"\"\n",
    "        Predict where the vehicle should be in the current frame (before seeing detections).\n",
    "        Stores prediction in self.last_pred for association gating.\n",
    "        \"\"\"\n",
    "        pred = self.kf.predict()\n",
    "        self.last_pred = (float(pred[0]), float(pred[1]))\n",
    "        return self.last_pred\n",
    "\n",
    "    def update(self, xy: tuple[int,int], bbox):\n",
    "        \"\"\"\n",
    "        Correct the predicted state using the detection measurement.\n",
    "        \"\"\"\n",
    "        cx, cy = xy\n",
    "        z = np.array([[cx], [cy]], dtype=np.float32)\n",
    "        self.kf.correct(z)\n",
    "        self.hits += 1\n",
    "        self.consecutive_hits += 1  # Increment consecutive hits\n",
    "        self.missed = 0\n",
    "        self.bbox = bbox\n",
    "        self.history.append(xy)\n",
    "\n",
    "    def mark_missed(self):\n",
    "        \"\"\"\n",
    "        No detection matched this track this frame\n",
    "        \"\"\"\n",
    "        self.missed += 1\n",
    "        self.consecutive_hits = 0  # Reset consecutive hits\n",
    "\n",
    "    def is_confirmed(self, min_hits: int = 3) -> bool:\n",
    "        \"\"\"Check if track is stable and should be displayed\"\"\"\n",
    "        return self.consecutive_hits >= min_hits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "dcc88691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adaptive association with measurement validation\n",
    "\n",
    "def match_distance_for_bbox(bbox):\n",
    "    \"\"\"\n",
    "    Larger objects can move more pixels between frames.\n",
    "    bbox height ≈ distance to camera (perspective effect).\n",
    "    Adaptive gate: closer objects (larger) can move more.\n",
    "    \"\"\"\n",
    "    _, _, w, h = bbox\n",
    "    return 20 + 0.8 * h   # slightly more permissive than before\n",
    "\n",
    "def euclidean_distance(track, detection):\n",
    "    \"\"\"\n",
    "    Compute Euclidean distance between track prediction and detection.\n",
    "    Uses predicted state (last_pred) for gating.\n",
    "    \"\"\"\n",
    "    pred_x, pred_y = track.last_pred\n",
    "    det_x, det_y = detection[\"centroid\"]\n",
    "    d = ((pred_x - det_x)**2 + (pred_y - det_y)**2) ** 0.5\n",
    "    return d\n",
    "\n",
    "\n",
    "def associate_detections_to_tracks(dets, tracks, min_confirmed_hits=3):\n",
    "    \"\"\"\n",
    "    For each track, pick the nearest detection,\n",
    "    but only if it lies within a size-dependent distance.\n",
    "    \n",
    "    Prioritize confirmed tracks to prevent ghost IDs from stealing detections.\n",
    "    Validate measurements: reject detections that would cause large corrections\n",
    "    to avoid mask artifacts corrupting track state.\n",
    "    \n",
    "    OPTIMIZED: Use sets instead of list.remove() to avoid O(n) operations.\n",
    "    \"\"\"\n",
    "\n",
    "    matches = []\n",
    "    unmatched_dets = set(range(len(dets)))\n",
    "    unmatched_tracks = set(range(len(tracks)))\n",
    "\n",
    "    # First pass: match CONFIRMED tracks (higher priority)\n",
    "    for ti, t in enumerate(tracks):\n",
    "        if not t.is_confirmed(min_confirmed_hits) or not unmatched_dets:\n",
    "            continue\n",
    "\n",
    "        #tx, ty = t.history[-1]\n",
    "        tx, ty = getattr(t, \"last_pred\", t.history[-1])\n",
    "        best_di = None\n",
    "        best_d = float(\"inf\")\n",
    "\n",
    "        for di in unmatched_dets:\n",
    "            cx, cy = dets[di][\"centroid\"]\n",
    "            d = ((tx - cx)**2 + (ty - cy)**2) ** 0.5\n",
    "            allowed = match_distance_for_bbox(dets[di][\"bbox\"])\n",
    "            \n",
    "\n",
    "            if d < allowed and d < best_d:\n",
    "                best_d = d\n",
    "                best_di = di\n",
    "\n",
    "        if best_di is not None:\n",
    "            # Additional validation: check that this detection is plausible\n",
    "            # using Kalman filter's predicted state \n",
    "            euclid_d = euclidean_distance(t, dets[best_di])\n",
    "            pred_allowed = match_distance_for_bbox(dets[best_di][\"bbox\"]) * 1.2  # 20% relaxation for KF\n",
    "            if euclid_d < pred_allowed:\n",
    "                matches.append((ti, best_di))\n",
    "                unmatched_dets.discard(best_di)\n",
    "                unmatched_tracks.discard(ti)\n",
    "\n",
    "    # Second pass: match UNCONFIRMED tracks (lower priority, stricter matching)\n",
    "    for ti, t in enumerate(tracks):\n",
    "        if t.is_confirmed(min_confirmed_hits) or ti not in unmatched_tracks or not unmatched_dets:\n",
    "            continue\n",
    "\n",
    "        #tx, ty = t.history[-1]\n",
    "        tx, ty = getattr(t, \"last_pred\", t.history[-1])\n",
    "        best_di = None\n",
    "        best_d = float(\"inf\")\n",
    "\n",
    "        for di in unmatched_dets:\n",
    "            cx, cy = dets[di][\"centroid\"]\n",
    "            d = ((tx - cx)**2 + (ty - cy)**2) ** 0.5\n",
    "            # Stricter gate for unconfirmed tracks (avoid spurious matches)\n",
    "            allowed = match_distance_for_bbox(dets[di][\"bbox\"]) * 0.7\n",
    "            \n",
    "\n",
    "            if d < allowed and d < best_d:\n",
    "                best_d = d\n",
    "                best_di = di\n",
    "\n",
    "        if best_di is not None:\n",
    "            matches.append((ti, best_di))\n",
    "            unmatched_dets.discard(best_di)\n",
    "            unmatched_tracks.discard(ti)\n",
    "\n",
    "    return matches, list(unmatched_tracks), list(unmatched_dets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "d502b59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper for visualization \n",
    "def draw_bbox(img, bbox, label, color, thickness=2):\n",
    "    x, y, w, h = bbox\n",
    "    x2, y2 = x + w, y + h\n",
    "\n",
    "    # optional \"shadow\" outline for contrast\n",
    "    cv2.rectangle(img, (x, y), (x2, y2), (0, 0, 0), thickness + 2, lineType=cv2.LINE_AA)\n",
    "    cv2.rectangle(img, (x, y), (x2, y2), color, thickness, lineType=cv2.LINE_AA)\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.55\n",
    "    text_thickness = 2\n",
    "    (tw, th), baseline = cv2.getTextSize(label, font, font_scale, text_thickness)\n",
    "\n",
    "    y_text_top = y - (th + baseline + 6)\n",
    "    if y_text_top < 0:\n",
    "        y_text_top = y + 2\n",
    "\n",
    "    x_text = x\n",
    "    y_text = y_text_top + th + 3\n",
    "\n",
    "    cv2.rectangle(img, (x_text, y_text_top), (x_text + tw + 8, y_text_top + th + baseline + 6),\n",
    "                  (0, 0, 0), -1, lineType=cv2.LINE_AA)\n",
    "    cv2.rectangle(img, (x_text + 1, y_text_top + 1), (x_text + tw + 7, y_text_top + th + baseline + 5),\n",
    "                  color, -1, lineType=cv2.LINE_AA)\n",
    "\n",
    "    cv2.putText(img, label, (x_text + 4, y_text),\n",
    "                font, font_scale, (0, 0, 0), text_thickness, lineType=cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e1d7df",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "The system follows a tracking by detection approach: first it detects moving vehicles in each frame, and then it keeps a persistent identity for each vehicle over time using a Kalman filter motion model.\n",
    "\n",
    "The tracker behaviour is controlled by a few parameters that determine how tolerant the system is to missing detections and motion uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "57eebe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_VIDEO = True\n",
    "VIDEO_NAME = \"vehicle_tracking_debug.mp4\"\n",
    "VIDEO_FPS = 20\n",
    "video_writer = None\n",
    "SHOW_EVERY = 1  \n",
    "SCALE = 0.4      # scale for display screen\n",
    "\n",
    "DT = 1.0\n",
    "MAX_AGE  = 30       # frames allowed to miss before deleting (allows recovery from mask gaps)\n",
    "MIN_HITS = 3         # show track after this many consecutive matches\n",
    "PROCESS_VAR = 0.2   \n",
    "MEAS_VAR    = 1.0\n",
    "# bacgkground subtractor object \n",
    "bg = cv2.createBackgroundSubtractorMOG2(history=400, varThreshold=15, detectShadows=True)\n",
    "\n",
    "tracks = []\n",
    "next_id = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c55206",
   "metadata": {},
   "source": [
    "\n",
    "`DT`  \n",
    "Represents the time step between frames in the motion model. In this project everything is measured per frame, so it is set to 1.0. It does not change the behaviour, it only keeps the motion equations consistent.\n",
    "\n",
    "`MAX_AGE`  \n",
    "Maximum number of consecutive frames a track can remain unmatched before being deleted. This allows a vehicle to temporarily disappear (for example due to glare or segmentation errors) and still keep its identity when it reappears. Larger values make the tracker more tolerant but may keep dead tracks alive longer.\n",
    "\n",
    "`MIN_HITS`  \n",
    "Number of consecutive successful matches required before a track is considered reliable. This prevents unstable short detections from immediately becoming tracked vehicles and reduces ID flickering.\n",
    "\n",
    "`PROCESS_VAR`  \n",
    "Indicates how uncertain the vehicle motion is assumed to be.  \n",
    "If it is large, the tracker assumes vehicles may change speed or direction and therefore relies more on the new detections.  \n",
    "If it is small, the tracker assumes motion is smooth and relies more on its predicted trajectory.\n",
    "\n",
    "`MEAS_VAR`  \n",
    "Indicates how noisy the detections are expected to be.  \n",
    "If it is large, the tracker considers the detections unreliable and follows the predicted trajectory more closely.  \n",
    "If it is small, the tracker follows the detections more strictly.\n",
    "\n",
    "In this project the detections come from background subtraction, which is noisy (shadows, glare and fragmentation).  \n",
    "Therefore the tracker is configured to trust the Kalman prediction more than the raw detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a64a5dc",
   "metadata": {},
   "source": [
    "\n",
    "# Main loop explanation \n",
    "\n",
    "For every frame of the video, the algorithm starts by extracting motion using a background subtraction model (MOG2). This produces a binary mask of moving regions. Because this raw mask contains noise, shadows and fragmented shapes, it is cleaned with morphological filtering so that each vehicle ideally becomes a single blob.\n",
    "\n",
    "From this cleaned mask, blobs are extracted and converted into detections. Each detection contains a centroid (the measurement used by the tracker) and a bounding box (used for validation and visualization). At this stage, the algorithm does not yet know which vehicle is which — it only knows where motion exists in the current frame.\n",
    "\n",
    "Next comes the prediction stage. Every tracked vehicle already has an associated Kalman filter that stores its estimated position and velocity. Before looking at the new detections, the tracker predicts where each vehicle should appear in the current frame. This prediction allows the system to bridge short detection failures (for example glare, shadows or imperfect segmentation).\n",
    "\n",
    "After prediction, detections are matched to tracks. The association is done in two passes: confirmed tracks are matched first (to protect stable identities), and unconfirmed tracks are matched afterwards with stricter conditions. A detection is only assigned to a track if it is spatially close enough to the predicted position. This distance gating prevents a vehicle from suddenly jumping to another lane or swapping identity with another car.\n",
    "\n",
    "When a match is found, the Kalman filter is corrected using the detected centroid. This step combines the prediction and the measurement to obtain a smoother and more stable estimate of the vehicle trajectory. If a track does not receive a detection in the current frame, it is not immediately deleted; instead, it is marked as “missed”. This allows the tracker to survive short occlusions or difficult lighting conditions.\n",
    "\n",
    "If a detection cannot be matched to any existing track, the system decides whether it represents a new vehicle or a temporarily lost one. To avoid creating duplicate identities, the detection is compared against all predicted track positions. The longer a track has been missing, the larger the allowed distance becomes. This adaptive suppression lets a vehicle disappear for several frames and still recover its original ID when it reappears.\n",
    "\n",
    "Only detections that are sufficiently far from all existing tracks create a new track with a new identifier. A track is considered reliable only after it has been successfully matched for several consecutive frames. Finally, tracks that remain unmatched for too long are removed from the system.\n",
    "\n",
    "Overall, the algorithm maintains stable vehicle identities by combining three ideas: motion detection to obtain measurements, a Kalman filter to predict motion over time, and adaptive association rules that tolerate temporary detection failures while avoiding duplicated IDs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "50c6255e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording video to: vehicle_tracking_debug.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sz/4qbyglh16n3gzj4l7_jkj3rh0000gn/T/ipykernel_8084/2664423280.py:67: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  self.last_pred = (float(pred[0]), float(pred[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: vehicle_tracking_debug.mp4\n"
     ]
    }
   ],
   "source": [
    "# Main loop TODO: explanation below can be deleted if not needed\n",
    "#For every frame, the Kalman filter predicts where the car should be (the predicted position).\n",
    "# When new detections (blobs) are found, the code tries to match them to existing tracks using both the last known position and the Kalman filter's prediction.\n",
    "#If a detection matches a track, the Kalman filter updates (corrects) its state using the detected position.\n",
    "\n",
    "\n",
    "for i, img_path in enumerate(IMAGE_FILES):\n",
    "    frame = cv2.imread(str(img_path))\n",
    "    if frame is None:\n",
    "        continue\n",
    "\n",
    "    H, W = frame.shape[:2]\n",
    "\n",
    "    # 1) Foreground mask (motion)\n",
    "    fg = bg.apply(frame)\n",
    "\n",
    "    # 2) Clean the mask\n",
    "    fg_clean = clean_mask(fg)\n",
    "\n",
    "    # 3) Detections = blobs (with merging of fragments)\n",
    "    dets = detect_blobs(fg_clean)\n",
    "\n",
    "    # 4) Predict all tracks (KF motion model)\n",
    "    for t in tracks:\n",
    "        t.predict()\n",
    "\n",
    "    # 5) Associate with two pass approach (confirmed tracks first)\n",
    "    matches, unmatched_tracks, unmatched_dets = associate_detections_to_tracks(\n",
    "        dets, tracks, min_confirmed_hits=MIN_HITS\n",
    "    )\n",
    "\n",
    "    # 6) Update matched tracks (KF correction step)\n",
    "    for ti, di in matches:\n",
    "        cx, cy = dets[di][\"centroid\"]\n",
    "        bbox = dets[di][\"bbox\"]\n",
    "        tracks[ti].update((cx, cy), bbox)\n",
    "\n",
    "    # 7) Mark unmatched tracks: we collect them in order to delete them later\n",
    "    for ti in unmatched_tracks:\n",
    "        tracks[ti].mark_missed()\n",
    "\n",
    "    # 8) Create new tracks for unmatched detections\n",
    "    for di in unmatched_dets:\n",
    "        cx, cy = dets[di][\"centroid\"]\n",
    "        bbox = dets[di][\"bbox\"]\n",
    "\n",
    "        # duplicate suppression: don't create new ID if near any existing track \n",
    "        duplicate = False\n",
    "        for t in tracks:\n",
    "            #tx, ty = t.history[-1]\n",
    "            tx, ty = getattr(t, \"last_pred\", t.history[-1]) # use prediction for better gating\n",
    "            dist = ((tx - cx)**2 + (ty - cy)**2) ** 0.5\n",
    "            base = 40 if not t.is_confirmed(MIN_HITS) else 50\n",
    "            # If the track has missed frames, enlarge the suppression radius\n",
    "            # Linear growth with a cap to avoid suppressing truly new vehicles\n",
    "            threshold = min(120, base + 20 * t.missed)\n",
    "            if dist < threshold:\n",
    "                duplicate = True\n",
    "                break\n",
    "        if duplicate:\n",
    "            continue\n",
    "\n",
    "        # Allow new track creation \n",
    "\n",
    "        tracks.append(Track(\n",
    "            track_id=next_id,\n",
    "            init_xy=(cx, cy),\n",
    "            init_bbox=bbox,\n",
    "            dt=DT,\n",
    "            process_var=PROCESS_VAR,\n",
    "            meas_var=MEAS_VAR\n",
    "        ))\n",
    "        next_id += 1\n",
    "\n",
    "    # 9) Delete only truly stale tracks (high MAX_AGE tolerance)\n",
    "    tracks = [t for t in tracks if t.missed <= MAX_AGE]\n",
    "\n",
    "    # 10) Visualize + record \n",
    "    if i % SHOW_EVERY == 0:\n",
    "        vis = frame.copy()\n",
    "\n",
    "        # Draw only confirmed tracks\n",
    "        for t in tracks:\n",
    "            if not t.is_confirmed(MIN_HITS):\n",
    "                continue\n",
    "            if t.missed > 1:\n",
    "                continue\n",
    "            label = f\"vehicle_{t.id}\"\n",
    "            draw_bbox(vis, t.bbox, label, t.color, thickness=2)\n",
    "\n",
    "        mask_vis = cv2.cvtColor(fg_clean, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        vis_small  = cv2.resize(vis, (int(W * SCALE), int(H * SCALE)))\n",
    "        mask_small = cv2.resize(mask_vis, (int(W * SCALE), int(H * SCALE)))\n",
    "\n",
    "        stacked = np.vstack([mask_small, vis_small])\n",
    "\n",
    "        # init writer \n",
    "        if SAVE_VIDEO and video_writer is None:\n",
    "            hh, ww = stacked.shape[:2]\n",
    "            fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "            video_writer = cv2.VideoWriter(VIDEO_NAME, fourcc, VIDEO_FPS, (ww, hh))\n",
    "            print(\"Recording video to:\", VIDEO_NAME)\n",
    "\n",
    "        if SAVE_VIDEO and video_writer is not None:\n",
    "            video_writer.write(stacked)\n",
    "\n",
    "        cv2.imshow(\"vehicle tracking\", stacked)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Cleanup\n",
    "if video_writer is not None:\n",
    "    video_writer.release()\n",
    "    print(\"Saved:\", VIDEO_NAME)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f3ba84",
   "metadata": {},
   "source": [
    "### challenges\n",
    "\n",
    "* multiple ids per car (duplicates)\n",
    "* id shifting/jumping \n",
    "* misses, we tried to find the perfect balance between reducin duplicates at the cost of misses and the opposite\n",
    "* a disadvantage of this method is that it is strongly tuned to the specific camera perspective\n",
    "* in our video theres a light area that causes a lot of missed trackings \n",
    "* car color affects the detection. theres one car that mixes with the lane in the first frames\n",
    "* also mog2 learns online so it needs a few frames to stabilize, this is a problem for our video cause its short"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef13b8e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sub11761",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
