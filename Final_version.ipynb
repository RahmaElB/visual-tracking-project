{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a464bde1-a3bb-4234-8a70-dc976c3f0cc8",
   "metadata": {},
   "source": [
    "# Project 2: Vehicle Tracking "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb7030b-9e00-4705-add7-22b6029bda35",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "#### Problem Description\n",
    "\n",
    "In this project, we address the problem of multi-vehicle tracking in a traffic video sequence. The objective is not only to detect moving vehicles in each frame, but also to maintain a consistent identity (ID) for each vehicle over time.\n",
    "\n",
    "Vehicle tracking is a fundamental problem in computer vision with applications in:\n",
    "\n",
    "- Traffic monitoring\n",
    "\n",
    "- Smart city infrastructure\n",
    "\n",
    "- Autonomous driving systems\n",
    "\n",
    "- Surveillance and safety analysis\n",
    "\n",
    "The main challenge is that detection alone is not sufficient. Vehicles must be tracked across frames even when:\n",
    "\n",
    "- Detection is noisy\n",
    "\n",
    "- Objects are partially occluded\n",
    "\n",
    "- Lighting conditions vary\n",
    "\n",
    "- Vehicles temporarily disappear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "78a341c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/Marta/.cache/kagglehub/datasets/trainingdatapro/cars-video-object-tracking/versions/3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"trainingdatapro/cars-video-object-tracking\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "DATASET_DIR = Path(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "c53758f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames: 301\n"
     ]
    }
   ],
   "source": [
    "IMAGE_DIR = DATASET_DIR / \"images\"\n",
    "\n",
    "assert IMAGE_DIR.exists(), f\"Missing {IMAGE_DIR}\"\n",
    "def sorted_images(folder: Path):\n",
    "    exts = (\".png\", \".jpg\", \".jpeg\", \".bmp\")\n",
    "    files = [p for p in folder.iterdir() if p.suffix.lower() in exts]\n",
    "    files.sort(key=lambda p: p.name)\n",
    "    return files\n",
    "\n",
    "IMAGE_FILES = sorted_images(IMAGE_DIR)\n",
    "\n",
    "print(\"Frames:\", len(IMAGE_FILES))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d81c0a",
   "metadata": {},
   "source": [
    "## Approach Overview\n",
    "\n",
    "We implement a tracking-by-detection pipeline, composed of two main stages:\n",
    "\n",
    "### Detection stage\n",
    "Vehicles are extracted from each frame using background subtraction (MOG2), mask refinement, geometric filtering, and blob clustering.\n",
    "\n",
    "### Tracking stage\n",
    "A Kalman Filter is used to:\n",
    "\n",
    "- Predict vehicle motion\n",
    "\n",
    "- Smooth noisy detections\n",
    "\n",
    "- Maintain stable vehicle identities over time\n",
    "\n",
    "The detection module provides imperfect measurements.\n",
    "The tracking module integrates them temporally to improve stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18095d20",
   "metadata": {},
   "source": [
    "## Detection\n",
    "\n",
    "#### Why Background Subtraction?\n",
    "\n",
    "We chose MOG2 (Mixture of Gaussians) because:\n",
    "\n",
    "- It is computationally efficient.\n",
    "\n",
    "- It adapts to gradual background changes.\n",
    "\n",
    "- It works well for fixed-camera traffic scenarios.\n",
    "\n",
    "Since the camera is static, moving vehicles appear as foreground regions, making background subtraction a suitable approach.\n",
    "\n",
    "This section extracts moving objects from video frames using background subtraction and morphological cleaning. We use the MOG2 algorithm to separate foreground (moving objects) from the static background. To remove noise, we apply morphological operations such as opening, closing, and dilation. We then use connected components to extract individual objects from the mask.\n",
    "\n",
    "Because the mask is imperfect due to various factors like vehicle color, distance to camera, and lighting conditions, a single car sometimes appears divided into multiple disconnected blobs. That's why we cluster and group the blobs that are likely to be part of the same vehicle, using proximity-based clustering to merge fragments while avoiding over-merging distant vehicles.\n",
    "\n",
    "### Functions\n",
    "\n",
    "Each function is explained below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97362004-254c-4efb-8844-2b193a713e15",
   "metadata": {},
   "source": [
    "#### Mask Cleaning \n",
    "\n",
    "The raw foreground mask may contain:\n",
    "\n",
    "- Shadows\n",
    "\n",
    "- Small noise regions\n",
    "\n",
    "- Fragmented vehicle shapes\n",
    "\n",
    "Processing steps:\n",
    "\n",
    "- Binary thresholding (remove weak shadow responses)\n",
    "\n",
    "- Morphological opening (3×3) → remove small noise\n",
    "\n",
    "- Morphological closing (2×2) → reconnect small gaps\n",
    "\n",
    "These operations reduce noise but do not produce perfectly segmented vehicles.\n",
    "\n",
    "#### Blob Extraction\n",
    "\n",
    "Contours are extracted from the cleaned mask.\n",
    "\n",
    "For each contour:\n",
    "\n",
    "- Area is computed\n",
    "\n",
    "- Bounding box is obtained\n",
    "\n",
    "- Centroid is calculated (used for tracking)\n",
    "\n",
    "Before accepting a blob as a vehicle candidate, geometric filtering is applied.\n",
    "\n",
    "#### Perspective-Aware Area Filtering\n",
    "\n",
    "Vehicle size varies with vertical position due to perspective.\n",
    "\n",
    "Let: t = y_bottom / H\n",
    "\n",
    "\n",
    "The expected area is modeled empirically as:\n",
    "\n",
    "Amin(t)=(2000+44749.12⋅t2)⋅kmin\n",
    "\n",
    "Amax(t)=(2000+108157.55⋅t2)⋅kmax\n",
    "\n",
    "where:\n",
    "\n",
    "- Kmin = 0.3\n",
    "- Kmax = 1.3\n",
    "\n",
    "The quadratic model was obtained empirically by observing vehicle sizes across image height in this scene.\n",
    "\n",
    "This improves consistency but is camera-dependent.\n",
    "\n",
    "#### Fragment Clustering \n",
    "\n",
    "Background subtraction may split one vehicle into multiple fragments.\n",
    "\n",
    "Two detections are grouped if:\n",
    "\n",
    "For each cluster:\n",
    "\n",
    "- Areas are summed\n",
    "\n",
    "- Centroid is area-weighted\n",
    "\n",
    "- Bounding box encloses all fragments\n",
    "\n",
    "This reduces duplicate detections but may still struggle when vehicles are very close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "ba291fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_mask(mask: np.ndarray) -> np.ndarray:\n",
    "    \n",
    "    _, mask = cv2.threshold(mask, 200, 255, cv2.THRESH_BINARY)\n",
    "    k_open = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    k_close = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2)) # note: this is a small value but works better for us than bigger values\n",
    "\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, k_open, iterations=2)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, k_close, iterations=2)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def cluster_blobs_by_proximity(dets: list, distance_threshold: float =100.0) -> list:\n",
    "    if len(dets) <= 1:\n",
    "        return dets\n",
    "    n = len(dets)\n",
    "    clusters = list(range(n))\n",
    "    def find_root(i):\n",
    "        if clusters[i] != i:\n",
    "            clusters[i] = find_root(clusters[i])\n",
    "        return clusters[i]\n",
    "    def union(i, j):\n",
    "        root_i, root_j = find_root(i), find_root(j)\n",
    "        if root_i != root_j:\n",
    "            clusters[root_j] = root_i\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            cx_i, cy_i = dets[i][\"centroid\"]\n",
    "            cx_j, cy_j = dets[j][\"centroid\"]\n",
    "            d = ((cx_j - cx_i)**2 + (cy_j - cy_i)**2) ** 0.5\n",
    "            if d < distance_threshold:\n",
    "                union(i, j)\n",
    "    # Merge clusters\n",
    "    cluster_map = {}\n",
    "    for i in range(n):\n",
    "        root = find_root(i)\n",
    "        if root not in cluster_map:\n",
    "            cluster_map[root] = []\n",
    "        cluster_map[root].append(i)\n",
    "    merged = []\n",
    "    for cluster_indices in cluster_map.values():\n",
    "        cluster_dets = [dets[i] for i in cluster_indices]\n",
    "        if len(cluster_dets) == 1:\n",
    "            merged.append(cluster_dets[0])\n",
    "        else:\n",
    "            total_area = sum(d[\"area\"] for d in cluster_dets)\n",
    "            merged_cx = sum(d[\"centroid\"][0] * d[\"area\"] for d in cluster_dets) / total_area\n",
    "            merged_cy = sum(d[\"centroid\"][1] * d[\"area\"] for d in cluster_dets) / total_area\n",
    "            all_xs = [d[\"bbox\"][0] for d in cluster_dets] + [d[\"bbox\"][0] + d[\"bbox\"][2] for d in cluster_dets]\n",
    "            all_ys = [d[\"bbox\"][1] for d in cluster_dets] + [d[\"bbox\"][1] + d[\"bbox\"][3] for d in cluster_dets]\n",
    "            x_min, x_max = min(all_xs), max(all_xs)\n",
    "            y_min, y_max = min(all_ys), max(all_ys)\n",
    "            merged.append({\n",
    "                \"centroid\": (int(merged_cx), int(merged_cy)),\n",
    "                \"bbox\": (x_min, y_min, x_max - x_min, y_max - y_min),\n",
    "                \"area\": total_area\n",
    "            })\n",
    "    return merged\n",
    "\n",
    "def allowed_area_range(y_bottom, img_h):\n",
    "\n",
    "    t = y_bottom / img_h\n",
    "    kmin = 0.3  # Adjusted to allow more variance, bigger c\n",
    "    kmax = 1.3\n",
    "    min_area = (2000.00 + 44749.12 * t * t) * kmin\n",
    "    max_area = (2000.00 + 108157.55 * t * t) * kmax\n",
    "    return min_area, max_area\n",
    "\n",
    "def detect_blobs(mask: np.ndarray):\n",
    "    H, W = mask.shape[:2]\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    dets = []\n",
    "    for c in contours:\n",
    "        area = cv2.contourArea(c)\n",
    "        if area <= 0:\n",
    "            continue\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "        y_bottom = y + h  # Use bottom of bbox for perspective model\n",
    "        minA, maxA = allowed_area_range(y_bottom, H)\n",
    "        if area <  minA or area > maxA:\n",
    "            continue\n",
    "        M = cv2.moments(c)\n",
    "        if M[\"m00\"] == 0:\n",
    "            continue\n",
    "        cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "        cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "        dets.append({\"centroid\": (cx, cy), \"bbox\": (x, y, w, h), \"area\": area})\n",
    "    # Cluster and merge nearby fragmented blobs\n",
    "    dets = cluster_blobs_by_proximity(dets, distance_threshold=40.0)\n",
    "    return dets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8df942",
   "metadata": {},
   "source": [
    "## Tracking\n",
    "\n",
    "Next, we need to track each detected car. We chose the Kalman filter because it is fast, supports multiple object tracking, and predicts the probable next position of each car based on its own motion model. Different cars move at different speeds, and the Kalman filter learns and adapts to these individual motion patterns.\n",
    "\n",
    "The Kalman filter maintains a state vector [x, y, vx, vy] representing position and velocity. The transition matrix defines how the state evolves between frames (it predicts where the car will be based on its current velocity). The measurement matrix maps the detected position (which we observe) to the state space. The process noise covariance controls how much we trust the motion model (lower values = trust motion more), while the measurement noise covariance controls how much we trust the detections (lower values = trust detections more). By balancing these, the filter smooths noisy detections while allowing the object to change speed.\n",
    "\n",
    "Each detection is attached to a Track object, which aims to maintain a persistent identity throughout the video. Every track has a unique ID and its own Kalman filter. The predict function estimates where the vehicle should be in the current frame using the motion model, before we see any new detections. The update function corrects the prediction using the actual detected position, allowing the filter to learn and adjust if the vehicle's motion changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c4ae1a-984d-44ab-88b1-9e1a0be43cf4",
   "metadata": {},
   "source": [
    "#### Track Lifecycle and State Management — Track Class\n",
    "\n",
    "The tracking behavior is controlled inside the Track class.\n",
    "\n",
    "Each track maintains:\n",
    "\n",
    "- hits → total successful matches\n",
    "\n",
    "- consecutive_hits → consecutive matched frames\n",
    "\n",
    "- missed → consecutive unmatched frames\n",
    "\n",
    "A track is confirmed if: consecutive_hits ≥ MIN_HITS\n",
    "\n",
    "This prevents short-lived detections from becoming stable identities.\n",
    "\n",
    "If a track is not matched in a frame:\n",
    "\n",
    "- missed is incremented\n",
    "\n",
    "- The Kalman prediction is kept as the current estimate\n",
    "\n",
    "A track is removed only if: missed > MAX_AGE\n",
    "\n",
    "These rules define when a track becomes reliable and when it is discarded.\n",
    "\n",
    "#### Adaptive Association Strategy \n",
    "\n",
    "Association is implemented in associate_detections_to_tracks() using adaptive distance gating.\n",
    "\n",
    "This heuristic accounts for perspective: closer vehicles (larger boxes) can move more pixels between frames.\n",
    "\n",
    "Matching is greedy (nearest-neighbor) and based on predicted track positions.\n",
    "\n",
    "#### Two-Pass Matching\n",
    "\n",
    "Association is performed in two passes:\n",
    "\n",
    "##### First Pass — Confirmed Tracks\n",
    "\n",
    "- Confirmed tracks are matched first\n",
    "\n",
    "- The closest detection within the adaptive gate is selected\n",
    "\n",
    "- A prediction-based validation step limits large corrections\n",
    "\n",
    "##### Second Pass — Unconfirmed Tracks\n",
    "\n",
    "- Remaining detections are matched\n",
    "\n",
    "- A stricter gate is applied\n",
    "\n",
    "This prioritizes stable identities while limiting unstable matches.\n",
    "\n",
    "#### Creation of New Tracks\n",
    "\n",
    "Unmatched detections are evaluated before creating a new Track.\n",
    "\n",
    "Each detection is compared with predicted positions of existing tracks using a suppression radius: r=min(120,base+20⋅missed)\n",
    "\n",
    "The radius increases with missed frames, allowing temporarily lost tracks to recover.\n",
    "\n",
    "Only detections sufficiently far from all predicted tracks generate a new ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "8718bbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_kalman(dt: float = 1.0, process_var=None, meas_var=None) -> cv2.KalmanFilter:\n",
    "    \"\"\"\n",
    "    State: [x, y, vx, vy]^T\n",
    "    Measurement: [x, y]^T\n",
    "    process_var, meas_var: empirical, in pixel units.\n",
    "    \"\"\"\n",
    "    kf = cv2.KalmanFilter(4, 2)\n",
    "\n",
    "    kf.transitionMatrix = np.array([\n",
    "        [1, 0, dt, 0 ],\n",
    "        [0, 1, 0 , dt],\n",
    "        [0, 0, 1 , 0 ],\n",
    "        [0, 0, 0 , 1 ],\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    kf.measurementMatrix = np.array([\n",
    "        [1, 0, 0, 0],\n",
    "        [0, 1, 0, 0],\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    # These are the two main tuning knobs.\n",
    "    kf.processNoiseCov = np.eye(4, dtype=np.float32) * process_var\n",
    "    kf.measurementNoiseCov = np.eye(2, dtype=np.float32) * meas_var\n",
    "\n",
    "    # Start uncertain so it can adapt quickly.\n",
    "    kf.errorCovPost = np.eye(4, dtype=np.float32) * 500.0\n",
    "    kf.statePost = np.zeros((4, 1), dtype=np.float32)\n",
    "    return kf\n",
    "\n",
    "def color_from_id(track_id: int) -> tuple[int, int, int]:\n",
    "\n",
    "    rng = np.random.default_rng(track_id)  # stable seed per id\n",
    "    # Keep colors away from extremes: [40..220]\n",
    "    c = rng.integers(40, 220, size=3, dtype=np.int32)\n",
    "    return (int(c[0]), int(c[1]), int(c[2]))  # B, G, R\n",
    "\n",
    "class Track:\n",
    "    \"\"\"\n",
    "    One vehicle hypothesis + identity.\n",
    "    We keep:\n",
    "      - Kalman filter\n",
    "      - hits: how many times we matched a detection (confidence)\n",
    "      - missed: how many consecutive frames we failed to match (death timer)\n",
    "      - confirmed: whether track has been stable for MIN_HITS frames\n",
    "    \"\"\"\n",
    "    def __init__(self, track_id: int, init_xy: tuple[int,int], init_bbox, dt=1.0, process_var=None, meas_var=None):\n",
    "        self.id = track_id\n",
    "        self.kf = init_kalman(dt=dt, process_var=process_var, meas_var=meas_var)\n",
    "        self.color = color_from_id(track_id)\n",
    "\n",
    "        x, y = init_xy\n",
    "        self.kf.statePost = np.array([[x], [y], [0], [0]], dtype=np.float32)\n",
    "\n",
    "        self.hits = 1\n",
    "        self.missed = 0\n",
    "        self.bbox = init_bbox\n",
    "        self.history = [init_xy]\n",
    "        self.consecutive_hits = 1  # Count consecutive frames with detections\n",
    "        self.last_pred = (x, y)  # Initialize with first position\n",
    "\n",
    "    def predict(self) -> tuple[float,float]:\n",
    "        \"\"\"\n",
    "        Predict where the vehicle should be in the current frame (before seeing detections).\n",
    "        Stores prediction in self.last_pred for association gating.\n",
    "        \"\"\"\n",
    "        pred = self.kf.predict()\n",
    "        self.last_pred = (float(pred[0]), float(pred[1]))\n",
    "        return self.last_pred\n",
    "\n",
    "    def update(self, xy: tuple[int,int], bbox):\n",
    "        \"\"\"\n",
    "        Correct the predicted state using the detection measurement.\n",
    "        \"\"\"\n",
    "        cx, cy = xy\n",
    "        z = np.array([[cx], [cy]], dtype=np.float32)\n",
    "        self.kf.correct(z)\n",
    "        self.hits += 1\n",
    "        self.consecutive_hits += 1  # Increment consecutive hits\n",
    "        self.missed = 0\n",
    "        self.bbox = bbox\n",
    "        self.history.append(xy)\n",
    "\n",
    "    def mark_missed(self):\n",
    "        \"\"\"\n",
    "        No detection matched this track this frame\n",
    "        \"\"\"\n",
    "        self.missed += 1\n",
    "        self.consecutive_hits = 0  # Reset consecutive hits\n",
    "\n",
    "    def is_confirmed(self, min_hits: int = 3) -> bool:\n",
    "        \"\"\"Check if track is stable and should be displayed\"\"\"\n",
    "        return self.consecutive_hits >= min_hits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "dcc88691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adaptive association with measurement validation\n",
    "\n",
    "def match_distance_for_bbox(bbox):\n",
    "    \"\"\"\n",
    "    Larger objects can move more pixels between frames.\n",
    "    bbox height ≈ distance to camera (perspective effect).\n",
    "    Adaptive gate: closer objects (larger) can move more.\n",
    "    \"\"\"\n",
    "    _, _, w, h = bbox\n",
    "    return 20 + 0.8 * h   # slightly more permissive than before\n",
    "\n",
    "def euclidean_distance(track, detection):\n",
    "    \"\"\"\n",
    "    Compute Euclidean distance between track prediction and detection.\n",
    "    Uses predicted state (last_pred) for gating.\n",
    "    \"\"\"\n",
    "    pred_x, pred_y = track.last_pred\n",
    "    det_x, det_y = detection[\"centroid\"]\n",
    "    d = ((pred_x - det_x)**2 + (pred_y - det_y)**2) ** 0.5\n",
    "    return d\n",
    "\n",
    "\n",
    "def associate_detections_to_tracks(dets, tracks, min_confirmed_hits=3):\n",
    "    \"\"\"\n",
    "    For each track, pick the nearest detection,\n",
    "    but only if it lies within a size-dependent distance.\n",
    "    \n",
    "    Prioritize confirmed tracks to prevent ghost IDs from stealing detections.\n",
    "    Validate measurements: reject detections that would cause large corrections\n",
    "    to avoid mask artifacts corrupting track state.\n",
    "    \n",
    "    OPTIMIZED: Use sets instead of list.remove() to avoid O(n) operations.\n",
    "    \"\"\"\n",
    "\n",
    "    matches = []\n",
    "    unmatched_dets = set(range(len(dets)))\n",
    "    unmatched_tracks = set(range(len(tracks)))\n",
    "\n",
    "    # First pass: match CONFIRMED tracks (higher priority)\n",
    "    for ti, t in enumerate(tracks):\n",
    "        if not t.is_confirmed(min_confirmed_hits) or not unmatched_dets:\n",
    "            continue\n",
    "\n",
    "        #tx, ty = t.history[-1]\n",
    "        tx, ty = getattr(t, \"last_pred\", t.history[-1])\n",
    "        best_di = None\n",
    "        best_d = float(\"inf\")\n",
    "\n",
    "        for di in unmatched_dets:\n",
    "            cx, cy = dets[di][\"centroid\"]\n",
    "            d = ((tx - cx)**2 + (ty - cy)**2) ** 0.5\n",
    "            allowed = match_distance_for_bbox(dets[di][\"bbox\"])\n",
    "            \n",
    "\n",
    "            if d < allowed and d < best_d:\n",
    "                best_d = d\n",
    "                best_di = di\n",
    "\n",
    "        if best_di is not None:\n",
    "            # Additional validation: check that this detection is plausible\n",
    "            # using Kalman filter's predicted state \n",
    "            euclid_d = euclidean_distance(t, dets[best_di])\n",
    "            pred_allowed = match_distance_for_bbox(dets[best_di][\"bbox\"]) * 1.2  # 20% relaxation for KF\n",
    "            if euclid_d < pred_allowed:\n",
    "                matches.append((ti, best_di))\n",
    "                unmatched_dets.discard(best_di)\n",
    "                unmatched_tracks.discard(ti)\n",
    "\n",
    "    # Second pass: match UNCONFIRMED tracks (lower priority, stricter matching)\n",
    "    for ti, t in enumerate(tracks):\n",
    "        if t.is_confirmed(min_confirmed_hits) or ti not in unmatched_tracks or not unmatched_dets:\n",
    "            continue\n",
    "\n",
    "        #tx, ty = t.history[-1]\n",
    "        tx, ty = getattr(t, \"last_pred\", t.history[-1])\n",
    "        best_di = None\n",
    "        best_d = float(\"inf\")\n",
    "\n",
    "        for di in unmatched_dets:\n",
    "            cx, cy = dets[di][\"centroid\"]\n",
    "            d = ((tx - cx)**2 + (ty - cy)**2) ** 0.5\n",
    "            # Stricter gate for unconfirmed tracks (avoid spurious matches)\n",
    "            allowed = match_distance_for_bbox(dets[di][\"bbox\"]) * 0.7\n",
    "            \n",
    "\n",
    "            if d < allowed and d < best_d:\n",
    "                best_d = d\n",
    "                best_di = di\n",
    "\n",
    "        if best_di is not None:\n",
    "            matches.append((ti, best_di))\n",
    "            unmatched_dets.discard(best_di)\n",
    "            unmatched_tracks.discard(ti)\n",
    "\n",
    "    return matches, list(unmatched_tracks), list(unmatched_dets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "d502b59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper for visualization \n",
    "def draw_bbox(img, bbox, label, color, thickness=2):\n",
    "    x, y, w, h = bbox\n",
    "    x2, y2 = x + w, y + h\n",
    "\n",
    "    # optional \"shadow\" outline for contrast\n",
    "    cv2.rectangle(img, (x, y), (x2, y2), (0, 0, 0), thickness + 2, lineType=cv2.LINE_AA)\n",
    "    cv2.rectangle(img, (x, y), (x2, y2), color, thickness, lineType=cv2.LINE_AA)\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.55\n",
    "    text_thickness = 2\n",
    "    (tw, th), baseline = cv2.getTextSize(label, font, font_scale, text_thickness)\n",
    "\n",
    "    y_text_top = y - (th + baseline + 6)\n",
    "    if y_text_top < 0:\n",
    "        y_text_top = y + 2\n",
    "\n",
    "    x_text = x\n",
    "    y_text = y_text_top + th + 3\n",
    "\n",
    "    cv2.rectangle(img, (x_text, y_text_top), (x_text + tw + 8, y_text_top + th + baseline + 6),\n",
    "                  (0, 0, 0), -1, lineType=cv2.LINE_AA)\n",
    "    cv2.rectangle(img, (x_text + 1, y_text_top + 1), (x_text + tw + 7, y_text_top + th + baseline + 5),\n",
    "                  color, -1, lineType=cv2.LINE_AA)\n",
    "\n",
    "    cv2.putText(img, label, (x_text + 4, y_text),\n",
    "                font, font_scale, (0, 0, 0), text_thickness, lineType=cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e1d7df",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "The system follows a tracking by detection approach: first it detects moving vehicles in each frame, and then it keeps a persistent identity for each vehicle over time using a Kalman filter motion model.\n",
    "\n",
    "The tracker behaviour is controlled by a few parameters that determine how tolerant the system is to missing detections and motion uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "57eebe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_VIDEO = True\n",
    "VIDEO_NAME = \"vehicle_tracking_debug.mp4\"\n",
    "VIDEO_FPS = 20\n",
    "video_writer = None\n",
    "SHOW_EVERY = 1  \n",
    "SCALE = 0.4      # scale for display screen\n",
    "\n",
    "DT = 1.0\n",
    "MAX_AGE  = 30       # frames allowed to miss before deleting (allows recovery from mask gaps)\n",
    "MIN_HITS = 3         # show track after this many consecutive matches\n",
    "PROCESS_VAR = 0.2   \n",
    "MEAS_VAR    = 1.0\n",
    "# bacgkground subtractor object \n",
    "bg = cv2.createBackgroundSubtractorMOG2(history=400, varThreshold=15, detectShadows=True)\n",
    "\n",
    "tracks = []\n",
    "next_id = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c55206",
   "metadata": {},
   "source": [
    "\n",
    "`DT`  \n",
    "Represents the time step between frames in the motion model. In this project everything is measured per frame, so it is set to 1.0. It does not change the behaviour, it only keeps the motion equations consistent.\n",
    "\n",
    "`MAX_AGE`  \n",
    "Maximum number of consecutive frames a track can remain unmatched before being deleted. This allows a vehicle to temporarily disappear (for example due to glare or segmentation errors) and still keep its identity when it reappears. Larger values make the tracker more tolerant but may keep dead tracks alive longer.\n",
    "\n",
    "`MIN_HITS`  \n",
    "Number of consecutive successful matches required before a track is considered reliable. This prevents unstable short detections from immediately becoming tracked vehicles and reduces ID flickering.\n",
    "\n",
    "`PROCESS_VAR`  \n",
    "Indicates how uncertain the vehicle motion is assumed to be.  \n",
    "If it is large, the tracker assumes vehicles may change speed or direction and therefore relies more on the new detections.  \n",
    "If it is small, the tracker assumes motion is smooth and relies more on its predicted trajectory.\n",
    "\n",
    "`MEAS_VAR`  \n",
    "Indicates how noisy the detections are expected to be.  \n",
    "If it is large, the tracker considers the detections unreliable and follows the predicted trajectory more closely.  \n",
    "If it is small, the tracker follows the detections more strictly.\n",
    "\n",
    "In this project the detections come from background subtraction, which is noisy (shadows, glare and fragmentation).  \n",
    "Therefore the tracker is configured to trust the Kalman prediction more than the raw detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a64a5dc",
   "metadata": {},
   "source": [
    "\n",
    "# Main loop explanation \n",
    "\n",
    "For every frame of the video, the algorithm starts by extracting motion using a background subtraction model (MOG2). This produces a binary mask of moving regions. Because this raw mask contains noise, shadows and fragmented shapes, it is cleaned with morphological filtering so that each vehicle ideally becomes a single blob.\n",
    "\n",
    "From this cleaned mask, blobs are extracted and converted into detections. Each detection contains a centroid (the measurement used by the tracker) and a bounding box (used for validation and visualization). At this stage, the algorithm does not yet know which vehicle is which — it only knows where motion exists in the current frame.\n",
    "\n",
    "Next comes the prediction stage. Every tracked vehicle already has an associated Kalman filter that stores its estimated position and velocity. Before looking at the new detections, the tracker predicts where each vehicle should appear in the current frame. This prediction allows the system to bridge short detection failures (for example glare, shadows or imperfect segmentation).\n",
    "\n",
    "After prediction, detections are matched to tracks. The association is done in two passes: confirmed tracks are matched first (to protect stable identities), and unconfirmed tracks are matched afterwards with stricter conditions. A detection is only assigned to a track if it is spatially close enough to the predicted position. This distance gating prevents a vehicle from suddenly jumping to another lane or swapping identity with another car.\n",
    "\n",
    "When a match is found, the Kalman filter is corrected using the detected centroid. This step combines the prediction and the measurement to obtain a smoother and more stable estimate of the vehicle trajectory. If a track does not receive a detection in the current frame, it is not immediately deleted; instead, it is marked as “missed”. This allows the tracker to survive short occlusions or difficult lighting conditions.\n",
    "\n",
    "If a detection cannot be matched to any existing track, the system decides whether it represents a new vehicle or a temporarily lost one. To avoid creating duplicate identities, the detection is compared against all predicted track positions. The longer a track has been missing, the larger the allowed distance becomes. This adaptive suppression lets a vehicle disappear for several frames and still recover its original ID when it reappears.\n",
    "\n",
    "Only detections that are sufficiently far from all existing tracks create a new track with a new identifier. A track is considered reliable only after it has been successfully matched for several consecutive frames. Finally, tracks that remain unmatched for too long are removed from the system.\n",
    "\n",
    "Overall, the algorithm maintains stable vehicle identities by combining three ideas: motion detection to obtain measurements, a Kalman filter to predict motion over time, and adaptive association rules that tolerate temporary detection failures while avoiding duplicated IDs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "50c6255e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording video to: vehicle_tracking_debug.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sz/4qbyglh16n3gzj4l7_jkj3rh0000gn/T/ipykernel_8084/2664423280.py:67: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  self.last_pred = (float(pred[0]), float(pred[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: vehicle_tracking_debug.mp4\n"
     ]
    }
   ],
   "source": [
    "#For every frame, the Kalman filter predicts where the car should be (the predicted position).\n",
    "# When new detections (blobs) are found, the code tries to match them to existing tracks using both the last known position and the Kalman filter's prediction.\n",
    "#If a detection matches a track, the Kalman filter updates (corrects) its state using the detected position.\n",
    "\n",
    "\n",
    "for i, img_path in enumerate(IMAGE_FILES):\n",
    "    frame = cv2.imread(str(img_path))\n",
    "    if frame is None:\n",
    "        continue\n",
    "\n",
    "    H, W = frame.shape[:2]\n",
    "\n",
    "    # 1) Foreground mask (motion)\n",
    "    fg = bg.apply(frame)\n",
    "\n",
    "    # 2) Clean the mask\n",
    "    fg_clean = clean_mask(fg)\n",
    "\n",
    "    # 3) Detections = blobs (with merging of fragments)\n",
    "    dets = detect_blobs(fg_clean)\n",
    "\n",
    "    # 4) Predict all tracks (KF motion model)\n",
    "    for t in tracks:\n",
    "        t.predict()\n",
    "\n",
    "    # 5) Associate with two pass approach (confirmed tracks first)\n",
    "    matches, unmatched_tracks, unmatched_dets = associate_detections_to_tracks(\n",
    "        dets, tracks, min_confirmed_hits=MIN_HITS\n",
    "    )\n",
    "\n",
    "    # 6) Update matched tracks (KF correction step)\n",
    "    for ti, di in matches:\n",
    "        cx, cy = dets[di][\"centroid\"]\n",
    "        bbox = dets[di][\"bbox\"]\n",
    "        tracks[ti].update((cx, cy), bbox)\n",
    "\n",
    "    # 7) Mark unmatched tracks: we collect them in order to delete them later\n",
    "    for ti in unmatched_tracks:\n",
    "        tracks[ti].mark_missed()\n",
    "\n",
    "    # 8) Create new tracks for unmatched detections\n",
    "    for di in unmatched_dets:\n",
    "        cx, cy = dets[di][\"centroid\"]\n",
    "        bbox = dets[di][\"bbox\"]\n",
    "\n",
    "        # duplicate suppression: don't create new ID if near any existing track \n",
    "        duplicate = False\n",
    "        for t in tracks:\n",
    "            #tx, ty = t.history[-1]\n",
    "            tx, ty = getattr(t, \"last_pred\", t.history[-1]) # use prediction for better gating\n",
    "            dist = ((tx - cx)**2 + (ty - cy)**2) ** 0.5\n",
    "            base = 40 if not t.is_confirmed(MIN_HITS) else 50\n",
    "            # If the track has missed frames, enlarge the suppression radius\n",
    "            # Linear growth with a cap to avoid suppressing truly new vehicles\n",
    "            threshold = min(120, base + 20 * t.missed)\n",
    "            if dist < threshold:\n",
    "                duplicate = True\n",
    "                break\n",
    "        if duplicate:\n",
    "            continue\n",
    "\n",
    "        # Allow new track creation \n",
    "\n",
    "        tracks.append(Track(\n",
    "            track_id=next_id,\n",
    "            init_xy=(cx, cy),\n",
    "            init_bbox=bbox,\n",
    "            dt=DT,\n",
    "            process_var=PROCESS_VAR,\n",
    "            meas_var=MEAS_VAR\n",
    "        ))\n",
    "        next_id += 1\n",
    "\n",
    "    # 9) Delete only truly stale tracks (high MAX_AGE tolerance)\n",
    "    tracks = [t for t in tracks if t.missed <= MAX_AGE]\n",
    "\n",
    "    # 10) Visualize + record \n",
    "    if i % SHOW_EVERY == 0:\n",
    "        vis = frame.copy()\n",
    "\n",
    "        # Draw only confirmed tracks\n",
    "        for t in tracks:\n",
    "            if not t.is_confirmed(MIN_HITS):\n",
    "                continue\n",
    "            if t.missed > 1:\n",
    "                continue\n",
    "            label = f\"vehicle_{t.id}\"\n",
    "            draw_bbox(vis, t.bbox, label, t.color, thickness=2)\n",
    "\n",
    "        mask_vis = cv2.cvtColor(fg_clean, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        vis_small  = cv2.resize(vis, (int(W * SCALE), int(H * SCALE)))\n",
    "        mask_small = cv2.resize(mask_vis, (int(W * SCALE), int(H * SCALE)))\n",
    "\n",
    "        stacked = np.vstack([mask_small, vis_small])\n",
    "\n",
    "        # init writer \n",
    "        if SAVE_VIDEO and video_writer is None:\n",
    "            hh, ww = stacked.shape[:2]\n",
    "            fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "            video_writer = cv2.VideoWriter(VIDEO_NAME, fourcc, VIDEO_FPS, (ww, hh))\n",
    "            print(\"Recording video to:\", VIDEO_NAME)\n",
    "\n",
    "        if SAVE_VIDEO and video_writer is not None:\n",
    "            video_writer.write(stacked)\n",
    "\n",
    "        cv2.imshow(\"vehicle tracking\", stacked)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Cleanup\n",
    "if video_writer is not None:\n",
    "    video_writer.release()\n",
    "    print(\"Saved:\", VIDEO_NAME)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f3ba84",
   "metadata": {},
   "source": [
    "## Challenges and Limitations\n",
    "\n",
    "- Duplicate IDs due to fragmentation or temporary loss\n",
    "\n",
    "- Identity switching when vehicles are close\n",
    "\n",
    "- Missed detections caused by glare or low contrast\n",
    "\n",
    "- Scene dependency of the perspective model\n",
    "\n",
    "- Lighting sensitivity of background subtraction\n",
    "\n",
    "- MOG2 warm-up instability in early frames\n",
    "\n",
    "## Possible Improvements\n",
    "\n",
    "- Incorporate appearance features\n",
    "\n",
    "- Use Hungarian algorithm for optimal assignment\n",
    "\n",
    "- Replace MOG2 with a deep detector (e.g., YOLO)\n",
    "\n",
    "- Apply shadow removal techniques\n",
    "\n",
    "- Adaptive parameter tuning\n",
    "\n",
    "- Add re-identification for long occlusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef13b8e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
