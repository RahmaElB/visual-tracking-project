{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a464bde1-a3bb-4234-8a70-dc976c3f0cc8",
   "metadata": {},
   "source": [
    "# Project 2: Vehicle Tracking "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb7030b-9e00-4705-add7-22b6029bda35",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "#### Problem Description\n",
    "\n",
    "In this project, we address the problem of multi-vehicle tracking in a traffic video sequence. The objective is not only to detect moving vehicles in each frame, but also to maintain a consistent identity (ID) for each vehicle over time.\n",
    "\n",
    "Vehicle tracking is a fundamental problem in computer vision with applications in:\n",
    "\n",
    "- Traffic monitoring\n",
    "\n",
    "- Smart city infrastructure\n",
    "\n",
    "- Autonomous driving systems\n",
    "\n",
    "- Surveillance and safety analysis\n",
    "\n",
    "The main challenge is that detection alone is not sufficient. Vehicles must be tracked across frames even when:\n",
    "\n",
    "- Detection is noisy\n",
    "\n",
    "- Objects are partially occluded\n",
    "\n",
    "- Lighting conditions vary\n",
    "\n",
    "- Vehicles temporarily disappear\n",
    "\n",
    "#### Video link\n",
    "\n",
    "https://drive.google.com/file/d/1bQPa8Yl_ufnt9xxYUh2o_OM3TCJ8KGTB/view?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78a341c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/Marta/.cache/kagglehub/datasets/trainingdatapro/cars-video-object-tracking/versions/3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"trainingdatapro/cars-video-object-tracking\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "DATASET_DIR = Path(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c53758f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames: 301\n"
     ]
    }
   ],
   "source": [
    "IMAGE_DIR = DATASET_DIR / \"images\"\n",
    "\n",
    "assert IMAGE_DIR.exists(), f\"Missing {IMAGE_DIR}\"\n",
    "def sorted_images(folder: Path):\n",
    "    exts = (\".png\", \".jpg\", \".jpeg\", \".bmp\")\n",
    "    files = [p for p in folder.iterdir() if p.suffix.lower() in exts]\n",
    "    files.sort(key=lambda p: p.name)\n",
    "    return files\n",
    "\n",
    "IMAGE_FILES = sorted_images(IMAGE_DIR)\n",
    "\n",
    "print(\"Frames:\", len(IMAGE_FILES))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d81c0a",
   "metadata": {},
   "source": [
    "## Approach Overview\n",
    "\n",
    "We implement a tracking-by-detection pipeline, composed of two main stages:\n",
    "\n",
    "### Detection stage\n",
    "Vehicles are extracted from each frame using background subtraction (MOG2) and morphological filtering.\n",
    "\n",
    "### Tracking stage\n",
    "A Kalman Filter is used to:\n",
    "\n",
    "- Predict vehicle motion\n",
    "\n",
    "- Smooth noisy detections\n",
    "\n",
    "- Maintain stable vehicle identities over time\n",
    "\n",
    "This combination allows us to handle imperfect segmentation while keeping trajectories consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18095d20",
   "metadata": {},
   "source": [
    "## Detection\n",
    "\n",
    "#### Why Background Subtraction?\n",
    "\n",
    "We chose MOG2 (Mixture of Gaussians) because:\n",
    "\n",
    "- It is computationally efficient.\n",
    "\n",
    "- It adapts to gradual background changes.\n",
    "\n",
    "- It works well for fixed-camera traffic scenarios.\n",
    "\n",
    "Since the camera is static, moving vehicles appear as foreground regions, making background subtraction a suitable approach.\n",
    "\n",
    "This section extracts moving objects from video frames using background subtraction and morphological cleaning. We use the MOG2 algorithm to separate foreground (moving objects) from the static background. To remove noise, we apply morphological operations such as opening, closing, and dilation. We then use connected components to extract individual objects from the mask.\n",
    "\n",
    "Because the mask is imperfect due to various factors like vehicle color, distance to camera, and lighting conditions, a single car sometimes appears divided into multiple disconnected blobs. That's why we cluster and group the blobs that are likely to be part of the same vehicle, using proximity-based clustering to merge fragments while avoiding over-merging distant vehicles.\n",
    "\n",
    "### Functions\n",
    "\n",
    "Each function is explained below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97362004-254c-4efb-8844-2b193a713e15",
   "metadata": {},
   "source": [
    "#### 1. Foreground Extraction\n",
    "\n",
    "Each frame is processed using the MOG2 background subtractor to obtain a foreground mask. In this binary mask:\n",
    "\n",
    "- Moving objects appear in white\n",
    "\n",
    "- Static background appears in black\n",
    "\n",
    "Since the camera is fixed, moving vehicles are detected as foreground regions.\n",
    "\n",
    "However, the raw mask produced by MOG2 is not perfect. It typically contains:\n",
    "\n",
    "- Shadows classified as foreground\n",
    "\n",
    "- Reflections and glare\n",
    "\n",
    "- Small noisy regions\n",
    "\n",
    "- Fragmented vehicle shapes\n",
    "\n",
    "Therefore, additional refinement steps are required before extracting reliable detections.\n",
    "\n",
    "#### 2. Mask Cleaning\n",
    "\n",
    "To improve the quality of the foreground mask, we apply several processing steps.\n",
    "\n",
    "##### Thresholding\n",
    "\n",
    "We first apply a binary threshold:\n",
    "\n",
    "- Pixel values above 200 → set to 255\n",
    "\n",
    "- All other values → set to 0\n",
    "\n",
    "This removes weak shadow responses and ensures a clean binary mask.\n",
    "\n",
    "##### Morphological Operations\n",
    "\n",
    "Two morphological operations are applied sequentially:\n",
    "\n",
    "###### Opening (elliptical kernel 3×3, 2 iterations)\n",
    "\n",
    "- Removes small isolated noise\n",
    "\n",
    "- Eliminates tiny false positives\n",
    "\n",
    "- Preserves main vehicle structures\n",
    "\n",
    "###### Closing (elliptical kernel 2×2, 2 iterations)\n",
    "\n",
    "- Fills small gaps inside blobs\n",
    "\n",
    "- Connects slightly fragmented regions\n",
    "\n",
    "We experimented with different kernel sizes during development. Larger kernels over-smoothed the mask and sometimes merged nearby vehicles. A small closing kernel (2×2) provided the best balance between noise removal and shape preservation.\n",
    "\n",
    "These parameters were selected empirically based on visual stability across frames.\n",
    "\n",
    "The result is a cleaner mask where each vehicle is more likely to appear as a single connected region.\n",
    "\n",
    "#### 3. Perspective-Aware Area Filtering\n",
    "\n",
    "Due to perspective projection:\n",
    "\n",
    "- Vehicles closer to the camera (bottom of the image) appear larger\n",
    "\n",
    "- Vehicles farther from the camera (top of the image) appear smaller\n",
    "\n",
    "To model this effect, we analysed the bounding box annotations of the dataset and studied how their pixel area grows along the vertical axis.  \n",
    "By plotting area vs vertical position we obtained a clear increasing trend and fitted two curves that estimate the expected minimum and maximum vehicle area as a function of height:\n",
    "\n",
    "For each detected blob:\n",
    "\n",
    "- The bounding box is computed\n",
    "- The bottom coordinate of the box (y_bottom) estimates its distance to the camera\n",
    "- The blob area is checked against this allowed range\n",
    "\n",
    "If the blob area is outside the interval, it is rejected.\n",
    "\n",
    "During testing, these limits were too strict because background subtraction often produces fragmented or slightly merged blobs.  \n",
    "To make the model tolerant to imperfect segmentation, we introduce scaling factors:\n",
    "\n",
    "- kmin = 0.3\n",
    "- kmax = 1.3\n",
    "\n",
    "These widen the acceptable area range and prevent rejecting valid vehicles due to:\n",
    "\n",
    "- Fragmentation\n",
    "- Slight merging\n",
    "- Lighting distortions\n",
    "\n",
    "Note: This approach is camera-dependent because the area model is tuned specifically to this viewpoint and scene geometry.\n",
    "\n",
    "#### 4. Blob Clustering\n",
    "\n",
    "Even after cleaning and filtering, background subtraction may produce multiple blobs for a single vehicle due to:\n",
    "\n",
    "- Broken masks\n",
    "\n",
    "- Gaps caused by reflections\n",
    "\n",
    "- Partial segmentation\n",
    "\n",
    "If left uncorrected, this would cause:\n",
    "\n",
    "- Duplicate track IDs\n",
    "\n",
    "- Jittery centroid measurements\n",
    "\n",
    "To address this, we implemented proximity-based clustering using a Union-Find (Disjoint Set) structure.\n",
    "\n",
    "Clustering Policy\n",
    "\n",
    "Each detection has a centroid.\n",
    "\n",
    "If two centroids are closer than a predefined distance threshold (40 pixels), they are grouped.\n",
    "\n",
    "Clustering is transitive:\n",
    "\n",
    "If A is close to B, and B is close to C, then all belong to the same cluster.\n",
    "\n",
    "##### Merging Strategy\n",
    "\n",
    "For each cluster, the final detection is computed as:\n",
    "\n",
    "- Area → sum of all member areas\n",
    "\n",
    "- Centroid → area-weighted average (larger fragments influence more)\n",
    "\n",
    "- Bounding box → tight bounding box enclosing all fragments\n",
    "\n",
    "This ensures that the merged detection accurately represents the full vehicle.\n",
    "\n",
    "##### Trade-Off\n",
    "\n",
    "The distance threshold controls a balance:\n",
    "\n",
    "- A larger threshold repairs more fragmentation but risks merging adjacent vehicles.\n",
    "\n",
    "- A smaller threshold avoids merging different vehicles but keeps some fragmentation.\n",
    "\n",
    "Through experimentation, a threshold of 40 pixels provided stable behavior in this scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba291fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_mask(mask: np.ndarray) -> np.ndarray:\n",
    "    \n",
    "    _, mask = cv2.threshold(mask, 200, 255, cv2.THRESH_BINARY)\n",
    "    k_open = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    k_close = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2)) \n",
    "    # note: this is a small value but works better for us than bigger values\n",
    "\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, k_open, iterations=2)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, k_close, iterations=2)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def cluster_blobs_by_proximity(dets: list, distance_threshold: float =100.0) -> list:\n",
    "    if len(dets) <= 1:\n",
    "        return dets\n",
    "    n = len(dets)\n",
    "    clusters = list(range(n))\n",
    "    def find_root(i):\n",
    "        if clusters[i] != i:\n",
    "            clusters[i] = find_root(clusters[i])\n",
    "        return clusters[i]\n",
    "    def union(i, j):\n",
    "        root_i, root_j = find_root(i), find_root(j)\n",
    "        if root_i != root_j:\n",
    "            clusters[root_j] = root_i\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            cx_i, cy_i = dets[i][\"centroid\"]\n",
    "            cx_j, cy_j = dets[j][\"centroid\"]\n",
    "            d = ((cx_j - cx_i)**2 + (cy_j - cy_i)**2) ** 0.5\n",
    "            if d < distance_threshold:\n",
    "                union(i, j)\n",
    "    # Merge clusters\n",
    "    cluster_map = {}\n",
    "    for i in range(n):\n",
    "        root = find_root(i)\n",
    "        if root not in cluster_map:\n",
    "            cluster_map[root] = []\n",
    "        cluster_map[root].append(i)\n",
    "    merged = []\n",
    "    for cluster_indices in cluster_map.values():\n",
    "        cluster_dets = [dets[i] for i in cluster_indices]\n",
    "        if len(cluster_dets) == 1:\n",
    "            merged.append(cluster_dets[0])\n",
    "        else:\n",
    "            total_area = sum(d[\"area\"] for d in cluster_dets)\n",
    "            merged_cx = sum(d[\"centroid\"][0] * d[\"area\"] for d in cluster_dets) / total_area\n",
    "            merged_cy = sum(d[\"centroid\"][1] * d[\"area\"] for d in cluster_dets) / total_area\n",
    "            all_xs = [d[\"bbox\"][0] for d in cluster_dets] + [d[\"bbox\"][0] + d[\"bbox\"][2] for d in cluster_dets]\n",
    "            all_ys = [d[\"bbox\"][1] for d in cluster_dets] + [d[\"bbox\"][1] + d[\"bbox\"][3] for d in cluster_dets]\n",
    "            x_min, x_max = min(all_xs), max(all_xs)\n",
    "            y_min, y_max = min(all_ys), max(all_ys)\n",
    "            merged.append({\n",
    "                \"centroid\": (int(merged_cx), int(merged_cy)),\n",
    "                \"bbox\": (x_min, y_min, x_max - x_min, y_max - y_min),\n",
    "                \"area\": total_area\n",
    "            })\n",
    "    return merged\n",
    "\n",
    "def allowed_area_range(y_bottom, img_h):\n",
    "    t = y_bottom / img_h\n",
    "    kmin = 0.3  # Adjusted to allow more variance\n",
    "    kmax = 1.3\n",
    "    min_area = (2000.00 + 44749.12 * t * t) * kmin\n",
    "    max_area = (2000.00 + 108157.55 * t * t) * kmax\n",
    "    return min_area, max_area\n",
    "\n",
    "def detect_blobs(mask: np.ndarray):\n",
    "    H, W = mask.shape[:2]\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    dets = []\n",
    "    for c in contours:\n",
    "        area = cv2.contourArea(c)\n",
    "        if area <= 0:\n",
    "            continue\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "        y_bottom = y + h  \n",
    "        minA, maxA = allowed_area_range(y_bottom, H)\n",
    "        if area <  minA or area > maxA:\n",
    "            continue\n",
    "        M = cv2.moments(c)\n",
    "        if M[\"m00\"] == 0:\n",
    "            continue\n",
    "        cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "        cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "        dets.append({\"centroid\": (cx, cy), \"bbox\": (x, y, w, h), \"area\": area})\n",
    "    # Cluster and merge nearby fragmented blobs\n",
    "    dets = cluster_blobs_by_proximity(dets, distance_threshold=40.0)\n",
    "    return dets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8df942",
   "metadata": {},
   "source": [
    "## Tracking\n",
    "\n",
    "Next, we need to track each detected car. We chose the Kalman filter because it is fast, supports multiple object tracking, and predicts the probable next position of each car based on its own motion model. Different cars move at different speeds, and the Kalman filter learns and adapts to these individual motion patterns.\n",
    "\n",
    "The Kalman filter maintains a state vector [x, y, vx, vy] representing position and velocity. The transition matrix defines how the state evolves between frames (it predicts where the car will be based on its current velocity). The measurement matrix maps the detected position (which we observe) to the state space. The process noise covariance controls how much we trust the motion model (lower values = trust motion more), while the measurement noise covariance controls how much we trust the detections (lower values = trust detections more). By balancing these, the filter smooths noisy detections while allowing the object to change speed.\n",
    "\n",
    "Each detection is attached to a Track object, which aims to maintain a persistent identity throughout the video. Every track has a unique ID and its own Kalman filter. The predict function estimates where the vehicle should be in the current frame using the motion model, before we see any new detections. The update function corrects the prediction using the actual detected position, allowing the filter to learn and adjust if the vehicle's motion changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c4ae1a-984d-44ab-88b1-9e1a0be43cf4",
   "metadata": {},
   "source": [
    "##### Track Lifecycle and State Management\n",
    "\n",
    "In addition to the Kalman filter, each Track object maintains internal counters to control its stability and lifetime:\n",
    "\n",
    "- hits → total number of successful matches\n",
    "\n",
    "- consecutive_hits → number of consecutive frames with successful detection\n",
    "\n",
    "- missed → number of consecutive frames without a detection\n",
    "\n",
    "A track is considered confirmed only after it has been matched for a minimum number of consecutive frames (MIN_HITS). This prevents short-lived noise detections from immediately becoming stable vehicle identities.\n",
    "\n",
    "If a track does not receive a detection in a given frame, it is not immediately removed. Instead:\n",
    "\n",
    "- The missed counter is incremented.\n",
    "\n",
    "- The track continues relying on its predicted position.\n",
    "\n",
    "A track is deleted only if:\n",
    "\n",
    "- missed > MAX_AGE\n",
    "\n",
    "This allows the system to tolerate temporary occlusions, glare, or segmentation failures while preventing stale tracks from persisting indefinitely.\n",
    "\n",
    "##### Adaptive Association Strategy\n",
    "\n",
    "Associating detections to existing tracks is one of the most critical parts of the tracking system.\n",
    "\n",
    "Instead of using a fixed distance threshold, we implement an adaptive gating strategy based on bounding box size:\n",
    "\n",
    "- Larger vehicles (closer to the camera) are allowed to move more pixels between frames.\n",
    "\n",
    "- Smaller vehicles (farther from the camera) are constrained by a stricter motion limit.\n",
    "\n",
    "The allowed matching distance is computed as:\n",
    "\n",
    "- 20 + 0.8 × bbox_height\n",
    "\n",
    "This makes the association perspective-aware and reduces identity switching across lanes.\n",
    "\n",
    "##### Two-Pass Matching Procedure\n",
    "\n",
    "To further improve robustness, association is performed in two passes.\n",
    "\n",
    "###### First Pass: Confirmed Tracks\n",
    "\n",
    "- Confirmed tracks are matched first.\n",
    "\n",
    "- This prevents stable identities from being “stolen” by newly appearing detections.\n",
    "\n",
    "- The closest detection within the adaptive gate is selected.\n",
    "\n",
    "Additionally, a validation step is performed using the Kalman prediction:\n",
    "\n",
    "- The Euclidean distance between the predicted position and the detection must remain plausible.\n",
    "\n",
    "- A slightly relaxed threshold is used for this validation.\n",
    "\n",
    "This prevents large, unrealistic corrections caused by mask artifacts or sudden segmentation errors.\n",
    "\n",
    "###### Second Pass: Unconfirmed Tracks\n",
    "\n",
    "After confirmed tracks are processed:\n",
    "\n",
    "- Unconfirmed tracks are matched.\n",
    "\n",
    "- A stricter distance threshold is applied.\n",
    "\n",
    "This reduces the risk of unstable or newly created tracks capturing incorrect detections.\n",
    "\n",
    "##### Creation of New Tracks\n",
    "\n",
    "If a detection cannot be matched to any existing track:\n",
    "\n",
    "- The system checks whether it is close to a predicted track position.\n",
    "\n",
    "- The suppression radius increases with the number of missed frames.\n",
    "\n",
    "This duplicate suppression mechanism prevents creating multiple IDs for the same vehicle if it temporarily disappears and reappears.\n",
    "\n",
    "Only detections sufficiently far from all predicted track positions result in the creation of a new Track object with a new unique ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8718bbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_kalman(dt: float = 1.0, process_var=None, meas_var=None) -> cv2.KalmanFilter:\n",
    "    \"\"\"\n",
    "    State: [x, y, vx, vy]^T\n",
    "    Measurement: [x, y]^T\n",
    "    process_var, meas_var: empirical, in pixel units.\n",
    "    \"\"\"\n",
    "    kf = cv2.KalmanFilter(4, 2)\n",
    "\n",
    "    kf.transitionMatrix = np.array([\n",
    "        [1, 0, dt, 0 ],\n",
    "        [0, 1, 0 , dt],\n",
    "        [0, 0, 1 , 0 ],\n",
    "        [0, 0, 0 , 1 ],\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    kf.measurementMatrix = np.array([\n",
    "        [1, 0, 0, 0],\n",
    "        [0, 1, 0, 0],\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    # These are the two main tuning knobs.\n",
    "    kf.processNoiseCov = np.eye(4, dtype=np.float32) * process_var\n",
    "    kf.measurementNoiseCov = np.eye(2, dtype=np.float32) * meas_var\n",
    "\n",
    "    # Start uncertain so it can adapt quickly.\n",
    "    kf.errorCovPost = np.eye(4, dtype=np.float32) * 500.0\n",
    "    kf.statePost = np.zeros((4, 1), dtype=np.float32)\n",
    "    return kf\n",
    "\n",
    "def color_from_id(track_id: int) -> tuple[int, int, int]:\n",
    "\n",
    "    rng = np.random.default_rng(track_id)  # stable seed per id\n",
    "    # Keep colors away from extremes: [40..220]\n",
    "    c = rng.integers(40, 220, size=3, dtype=np.int32)\n",
    "    return (int(c[0]), int(c[1]), int(c[2]))  # B, G, R\n",
    "\n",
    "class Track:\n",
    "    \"\"\"\n",
    "    One vehicle hypothesis + identity.\n",
    "    We keep:\n",
    "      - Kalman filter\n",
    "      - hits: how many times we matched a detection (confidence)\n",
    "      - missed: how many consecutive frames we failed to match (death timer)\n",
    "      - confirmed: whether track has been stable for MIN_HITS frames\n",
    "    \"\"\"\n",
    "    def __init__(self, track_id: int, init_xy: tuple[int,int], init_bbox, dt=1.0, process_var=None, meas_var=None):\n",
    "        self.id = track_id\n",
    "        self.kf = init_kalman(dt=dt, process_var=process_var, meas_var=meas_var)\n",
    "        self.color = color_from_id(track_id)\n",
    "\n",
    "        x, y = init_xy\n",
    "        self.kf.statePost = np.array([[x], [y], [0], [0]], dtype=np.float32)\n",
    "\n",
    "        self.hits = 1\n",
    "        self.missed = 0\n",
    "        self.bbox = init_bbox\n",
    "        self.history = [init_xy]\n",
    "        self.consecutive_hits = 1  # Count consecutive frames with detections\n",
    "        self.last_pred = (x, y)  # Initialize with first position\n",
    "\n",
    "    def predict(self) -> tuple[float,float]:\n",
    "        \"\"\"\n",
    "        Predict where the vehicle should be in the current frame (before seeing detections).\n",
    "        Stores prediction in self.last_pred for association gating.\n",
    "        \"\"\"\n",
    "        pred = self.kf.predict()\n",
    "        self.last_pred = (float(pred[0]), float(pred[1]))\n",
    "        return self.last_pred\n",
    "\n",
    "    def update(self, xy: tuple[int,int], bbox):\n",
    "        \"\"\"\n",
    "        Correct the predicted state using the detection measurement.\n",
    "        \"\"\"\n",
    "        cx, cy = xy\n",
    "        z = np.array([[cx], [cy]], dtype=np.float32)\n",
    "        self.kf.correct(z)\n",
    "        self.hits += 1\n",
    "        self.consecutive_hits += 1  # Increment consecutive hits\n",
    "        self.missed = 0\n",
    "        self.bbox = bbox\n",
    "        self.history.append(xy)\n",
    "\n",
    "    def mark_missed(self):\n",
    "        \"\"\"\n",
    "        No detection matched this track this frame\n",
    "        \"\"\"\n",
    "        self.missed += 1\n",
    "        self.consecutive_hits = 0  # Reset consecutive hits\n",
    "\n",
    "    def is_confirmed(self, min_hits: int = 3) -> bool:\n",
    "        \"\"\"Check if track is stable and should be displayed\"\"\"\n",
    "        return self.consecutive_hits >= min_hits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcc88691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adaptive association with measurement validation\n",
    "\n",
    "def match_distance_for_bbox(bbox):\n",
    "    \"\"\"\n",
    "    Larger objects can move more pixels between frames\n",
    "    bbox height = distance to camera (perspective effect)\n",
    "    Adaptive gate: closer objects (larger) can move more\n",
    "    \"\"\"\n",
    "    _, _, w, h = bbox\n",
    "    return 20 + 0.8 * h   # slightly more permissive than before\n",
    "\n",
    "def euclidean_distance(track, detection):\n",
    "    \"\"\"\n",
    "    Compute Euclidean distance between track prediction and detection.\n",
    "    Uses predicted state (last_pred) for gating\n",
    "    \"\"\"\n",
    "    pred_x, pred_y = track.last_pred\n",
    "    det_x, det_y = detection[\"centroid\"]\n",
    "    d = ((pred_x - det_x)**2 + (pred_y - det_y)**2) ** 0.5\n",
    "    return d\n",
    "\n",
    "\n",
    "def associate_detections_to_tracks(dets, tracks, min_confirmed_hits=3):\n",
    "    \"\"\"\n",
    "    For each track, pick the nearest detection,\n",
    "    but only if it lies within a size-dependent distance.\n",
    "    \n",
    "    Prioritize confirmed tracks to prevent ghost IDs from stealing detections.\n",
    "    Validate measurements: reject detections that would cause large corrections\n",
    "    to avoid mask artifacts corrupting track state.\n",
    "    \n",
    "    OPTIMIZED: Use sets instead of list.remove() to avoid O(n) operations.\n",
    "    \"\"\"\n",
    "\n",
    "    matches = []\n",
    "    unmatched_dets = set(range(len(dets)))\n",
    "    unmatched_tracks = set(range(len(tracks)))\n",
    "\n",
    "    # First pass: match CONFIRMED tracks (higher priority)\n",
    "    for ti, t in enumerate(tracks):\n",
    "        if not t.is_confirmed(min_confirmed_hits) or not unmatched_dets:\n",
    "            continue\n",
    "\n",
    "        #tx, ty = t.history[-1]\n",
    "        tx, ty = getattr(t, \"last_pred\", t.history[-1])\n",
    "        best_di = None\n",
    "        best_d = float(\"inf\")\n",
    "\n",
    "        for di in unmatched_dets:\n",
    "            cx, cy = dets[di][\"centroid\"]\n",
    "            d = ((tx - cx)**2 + (ty - cy)**2) ** 0.5\n",
    "            allowed = match_distance_for_bbox(dets[di][\"bbox\"])\n",
    "            \n",
    "\n",
    "            if d < allowed and d < best_d:\n",
    "                best_d = d\n",
    "                best_di = di\n",
    "\n",
    "        if best_di is not None:\n",
    "            # Additional validation: check that this detection is plausible\n",
    "            # using Kalman filter's predicted state \n",
    "            euclid_d = euclidean_distance(t, dets[best_di])\n",
    "            pred_allowed = match_distance_for_bbox(dets[best_di][\"bbox\"]) * 1.2  # 20% relaxation for KF\n",
    "            if euclid_d < pred_allowed:\n",
    "                matches.append((ti, best_di))\n",
    "                unmatched_dets.discard(best_di)\n",
    "                unmatched_tracks.discard(ti)\n",
    "\n",
    "    # Second pass: match UNCONFIRMED tracks (lower priority, stricter matching)\n",
    "    for ti, t in enumerate(tracks):\n",
    "        if t.is_confirmed(min_confirmed_hits) or ti not in unmatched_tracks or not unmatched_dets:\n",
    "            continue\n",
    "\n",
    "        #tx, ty = t.history[-1]\n",
    "        tx, ty = getattr(t, \"last_pred\", t.history[-1])\n",
    "        best_di = None\n",
    "        best_d = float(\"inf\")\n",
    "\n",
    "        for di in unmatched_dets:\n",
    "            cx, cy = dets[di][\"centroid\"]\n",
    "            d = ((tx - cx)**2 + (ty - cy)**2) ** 0.5\n",
    "            # Stricter gate for unconfirmed tracks (avoid spurious matches)\n",
    "            allowed = match_distance_for_bbox(dets[di][\"bbox\"]) * 0.7\n",
    "            \n",
    "\n",
    "            if d < allowed and d < best_d:\n",
    "                best_d = d\n",
    "                best_di = di\n",
    "\n",
    "        if best_di is not None:\n",
    "            matches.append((ti, best_di))\n",
    "            unmatched_dets.discard(best_di)\n",
    "            unmatched_tracks.discard(ti)\n",
    "\n",
    "    return matches, list(unmatched_tracks), list(unmatched_dets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d502b59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper for visualization \n",
    "def draw_bbox(img, bbox, label, color, thickness=2):\n",
    "    x, y, w, h = bbox\n",
    "    x2, y2 = x + w, y + h\n",
    "\n",
    "    # optional \"shadow\" outline for contrast\n",
    "    cv2.rectangle(img, (x, y), (x2, y2), (0, 0, 0), thickness + 2, lineType=cv2.LINE_AA)\n",
    "    cv2.rectangle(img, (x, y), (x2, y2), color, thickness, lineType=cv2.LINE_AA)\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.55\n",
    "    text_thickness = 2\n",
    "    (tw, th), baseline = cv2.getTextSize(label, font, font_scale, text_thickness)\n",
    "\n",
    "    y_text_top = y - (th + baseline + 6)\n",
    "    if y_text_top < 0:\n",
    "        y_text_top = y + 2\n",
    "\n",
    "    x_text = x\n",
    "    y_text = y_text_top + th + 3\n",
    "\n",
    "    cv2.rectangle(img, (x_text, y_text_top), (x_text + tw + 8, y_text_top + th + baseline + 6),\n",
    "                  (0, 0, 0), -1, lineType=cv2.LINE_AA)\n",
    "    cv2.rectangle(img, (x_text + 1, y_text_top + 1), (x_text + tw + 7, y_text_top + th + baseline + 5),\n",
    "                  color, -1, lineType=cv2.LINE_AA)\n",
    "\n",
    "    cv2.putText(img, label, (x_text + 4, y_text),\n",
    "                font, font_scale, (0, 0, 0), text_thickness, lineType=cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e1d7df",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "The system follows a tracking by detection approach: first it detects moving vehicles in each frame, and then it keeps a persistent identity for each vehicle over time using a Kalman filter motion model.\n",
    "\n",
    "The tracker behaviour is controlled by a few parameters that determine how tolerant the system is to missing detections and motion uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57eebe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_VIDEO = True\n",
    "VIDEO_NAME = \"vehicle_tracking_debug.mp4\"\n",
    "VIDEO_FPS = 20\n",
    "video_writer = None\n",
    "SHOW_EVERY = 1  \n",
    "SCALE = 0.4      # scale for display screen\n",
    "\n",
    "DT = 1.0\n",
    "MAX_AGE  = 30       # frames allowed to miss before deleting (allows recovery from mask gaps)\n",
    "MIN_HITS = 3         # show track after this many consecutive matches\n",
    "PROCESS_VAR = 0.2   \n",
    "MEAS_VAR    = 1.0\n",
    "# bacgkground subtractor object \n",
    "bg = cv2.createBackgroundSubtractorMOG2(history=400, varThreshold=15, detectShadows=True)\n",
    "\n",
    "tracks = []\n",
    "next_id = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c55206",
   "metadata": {},
   "source": [
    "\n",
    "`DT`  \n",
    "Represents the time step between frames in the motion model. In this project everything is measured per frame, so it is set to 1.0. It does not change the behaviour, it only keeps the motion equations consistent.\n",
    "\n",
    "`MAX_AGE`  \n",
    "Maximum number of consecutive frames a track can remain unmatched before being deleted. This allows a vehicle to temporarily disappear (for example due to glare or segmentation errors) and still keep its identity when it reappears. Larger values make the tracker more tolerant but may keep dead tracks alive longer.\n",
    "\n",
    "`MIN_HITS`  \n",
    "Number of consecutive successful matches required before a track is considered reliable. This prevents unstable short detections from immediately becoming tracked vehicles and reduces ID flickering.\n",
    "\n",
    "`PROCESS_VAR`  \n",
    "Indicates how uncertain the vehicle motion is assumed to be.  \n",
    "If it is large, the tracker assumes vehicles may change speed or direction and therefore relies more on the new detections.  \n",
    "If it is small, the tracker assumes motion is smooth and relies more on its predicted trajectory.\n",
    "\n",
    "`MEAS_VAR`  \n",
    "Indicates how noisy the detections are expected to be.  \n",
    "If it is large, the tracker considers the detections unreliable and follows the predicted trajectory more closely.  \n",
    "If it is small, the tracker follows the detections more strictly.\n",
    "\n",
    "In this project the detections come from background subtraction, which is noisy (shadows, glare and fragmentation).  \n",
    "Therefore the tracker is configured to trust the Kalman prediction more than the raw detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a64a5dc",
   "metadata": {},
   "source": [
    "\n",
    "# Main loop explanation \n",
    "\n",
    "For every frame of the video, the algorithm starts by extracting motion using a background subtraction model (MOG2). This produces a binary mask of moving regions. Because this raw mask contains noise, shadows and fragmented shapes, it is cleaned with morphological filtering so that each vehicle ideally becomes a single blob.\n",
    "\n",
    "From this cleaned mask, blobs are extracted and converted into detections. Each detection contains a centroid (the measurement used by the tracker) and a bounding box (used for validation and visualization). At this stage, the algorithm does not yet know which vehicle is which, it only knows where motion exists in the current frame.\n",
    "\n",
    "Next comes the prediction stage. Every tracked vehicle already has an associated Kalman filter that stores its estimated position and velocity. Before looking at the new detections, the tracker predicts where each vehicle should appear in the current frame. This prediction allows the system to bridge short detection failures (for example glare, shadows or imperfect segmentation).\n",
    "\n",
    "After prediction, detections are matched to tracks. The association is done in two passes: confirmed tracks are matched first (to protect stable identities), and unconfirmed tracks are matched afterwards with stricter conditions. A detection is only assigned to a track if it is spatially close enough to the predicted position. This distance gating prevents a vehicle from suddenly jumping to another lane or swapping identity with another car.\n",
    "\n",
    "When a match is found, the Kalman filter is corrected using the detected centroid. This step combines the prediction and the measurement to obtain a smoother and more stable estimate of the vehicle trajectory. If a track does not receive a detection in the current frame, it is not immediately deleted; instead, it is marked as “missed”. This allows the tracker to survive short occlusions or difficult lighting conditions.\n",
    "\n",
    "If a detection cannot be matched to any existing track, the system decides whether it represents a new vehicle or a temporarily lost one. To avoid creating duplicate identities, the detection is compared against all predicted track positions. The longer a track has been missing, the larger the allowed distance becomes. This adaptive suppression lets a vehicle disappear for several frames and still recover its original ID when it reappears.\n",
    "\n",
    "Only detections that are sufficiently far from all existing tracks create a new track with a new identifier. A track is considered reliable only after it has been successfully matched for several consecutive frames. Finally, tracks that remain unmatched for too long are removed from the system.\n",
    "\n",
    "Overall, the algorithm maintains stable vehicle identities by combining three ideas: motion detection to obtain measurements, a Kalman filter to predict motion over time, and adaptive association rules that tolerate temporary detection failures while avoiding duplicated IDs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50c6255e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording video to: vehicle_tracking_debug.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sz/4qbyglh16n3gzj4l7_jkj3rh0000gn/T/ipykernel_30849/2664423280.py:67: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  self.last_pred = (float(pred[0]), float(pred[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: vehicle_tracking_debug.mp4\n"
     ]
    }
   ],
   "source": [
    "#For every frame, the Kalman filter predicts where the car should be (the predicted position).\n",
    "# When new detections (blobs) are found, the code tries to match them to existing tracks using both the last known position and the Kalman filter's prediction.\n",
    "#If a detection matches a track, the Kalman filter updates (corrects) its state using the detected position.\n",
    "\n",
    "\n",
    "for i, img_path in enumerate(IMAGE_FILES):\n",
    "    frame = cv2.imread(str(img_path))\n",
    "    if frame is None:\n",
    "        continue\n",
    "\n",
    "    H, W = frame.shape[:2]\n",
    "\n",
    "    # 1) Foreground mask (motion)\n",
    "    fg = bg.apply(frame)\n",
    "\n",
    "    # 2) Clean the mask\n",
    "    fg_clean = clean_mask(fg)\n",
    "\n",
    "    # 3) Detections = blobs (with merging of fragments)\n",
    "    dets = detect_blobs(fg_clean)\n",
    "\n",
    "    # 4) Predict all tracks (KF motion model)\n",
    "    for t in tracks:\n",
    "        t.predict()\n",
    "\n",
    "    # 5) Associate with two pass approach (confirmed tracks first)\n",
    "    matches, unmatched_tracks, unmatched_dets = associate_detections_to_tracks(\n",
    "        dets, tracks, min_confirmed_hits=MIN_HITS\n",
    "    )\n",
    "\n",
    "    # 6) Update matched tracks (KF correction step)\n",
    "    for ti, di in matches:\n",
    "        cx, cy = dets[di][\"centroid\"]\n",
    "        bbox = dets[di][\"bbox\"]\n",
    "        tracks[ti].update((cx, cy), bbox)\n",
    "\n",
    "    # 7) Mark unmatched tracks: we collect them in order to delete them later\n",
    "    for ti in unmatched_tracks:\n",
    "        tracks[ti].mark_missed()\n",
    "\n",
    "    # 8) Create new tracks for unmatched detections\n",
    "    for di in unmatched_dets:\n",
    "        cx, cy = dets[di][\"centroid\"]\n",
    "        bbox = dets[di][\"bbox\"]\n",
    "\n",
    "        # duplicate suppression: don't create new ID if near any existing track \n",
    "        duplicate = False\n",
    "        for t in tracks:\n",
    "            #tx, ty = t.history[-1]\n",
    "            tx, ty = getattr(t, \"last_pred\", t.history[-1]) # use prediction \n",
    "            dist = ((tx - cx)**2 + (ty - cy)**2) ** 0.5\n",
    "            base = 40 if not t.is_confirmed(MIN_HITS) else 50\n",
    "            # If the track has missed frames, enlarge the suppression radius\n",
    "            # Linear growth with a cap to avoid suppressing truly new vehicles\n",
    "            threshold = min(120, base + 20 * t.missed)\n",
    "            if dist < threshold:\n",
    "                duplicate = True\n",
    "                break\n",
    "        if duplicate:\n",
    "            continue\n",
    "\n",
    "        # Allow new track creation \n",
    "\n",
    "        tracks.append(Track(\n",
    "            track_id=next_id,\n",
    "            init_xy=(cx, cy),\n",
    "            init_bbox=bbox,\n",
    "            dt=DT,\n",
    "            process_var=PROCESS_VAR,\n",
    "            meas_var=MEAS_VAR\n",
    "        ))\n",
    "        next_id += 1\n",
    "\n",
    "    # 9) Delete only truly stale tracks (high MAX_AGE tolerance)\n",
    "    tracks = [t for t in tracks if t.missed <= MAX_AGE]\n",
    "\n",
    "    # 10) Visualize + record \n",
    "    if i % SHOW_EVERY == 0:\n",
    "        vis = frame.copy()\n",
    "\n",
    "        # Draw only confirmed tracks\n",
    "        for t in tracks:\n",
    "            if not t.is_confirmed(MIN_HITS):\n",
    "                continue\n",
    "            if t.missed > MAX_AGE:\n",
    "                continue\n",
    "            label = f\"vehicle_{t.id}\"\n",
    "            draw_bbox(vis, t.bbox, label, t.color, thickness=2)\n",
    "\n",
    "        mask_vis = cv2.cvtColor(fg_clean, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        vis_small  = cv2.resize(vis, (int(W * SCALE), int(H * SCALE)))\n",
    "        mask_small = cv2.resize(mask_vis, (int(W * SCALE), int(H * SCALE)))\n",
    "\n",
    "        stacked = np.vstack([mask_small, vis_small])\n",
    "\n",
    "        # init writer \n",
    "        if SAVE_VIDEO and video_writer is None:\n",
    "            hh, ww = stacked.shape[:2]\n",
    "            fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "            video_writer = cv2.VideoWriter(VIDEO_NAME, fourcc, VIDEO_FPS, (ww, hh))\n",
    "            print(\"Recording video to:\", VIDEO_NAME)\n",
    "\n",
    "        if SAVE_VIDEO and video_writer is not None:\n",
    "            video_writer.write(stacked)\n",
    "\n",
    "        cv2.imshow(\"vehicle tracking\", stacked)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Cleanup\n",
    "if video_writer is not None:\n",
    "    video_writer.release()\n",
    "    print(\"Saved:\", VIDEO_NAME)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f3ba84",
   "metadata": {},
   "source": [
    "## Challenges and Possible Improvements\n",
    "### Challenges\n",
    "\n",
    "During the implementation and testing of the tracking system, several practical challenges were encountered.\n",
    "\n",
    "#### 1. Multiple IDs per Vehicle (Duplicate Tracks)\n",
    "\n",
    "One of the main issues was the creation of multiple track IDs for the same vehicle.\n",
    "\n",
    "This typically occurred when:\n",
    "\n",
    "- Background subtraction fragmented a single vehicle into multiple blobs.\n",
    "\n",
    "- A vehicle temporarily disappeared and reappeared.\n",
    "\n",
    "- The association gate was too strict.\n",
    "\n",
    "Although blob clustering and duplicate suppression reduced this issue, there is always a trade-off:\n",
    "\n",
    "- If the matching threshold is too strict → more duplicate IDs appear.\n",
    "\n",
    "- If the threshold is too relaxed → different vehicles may merge or swap identities.\n",
    "\n",
    "Finding the correct balance required empirical tuning.\n",
    "\n",
    "#### 2. ID Switching / ID Jumping\n",
    "\n",
    "When vehicles move close to each other (e.g., adjacent lanes), association becomes ambiguous.\n",
    "\n",
    "Even with adaptive distance gating, the tracker may:\n",
    "\n",
    "- Assign a detection to the wrong track.\n",
    "\n",
    "- Cause identity switching between vehicles.\n",
    "\n",
    "This is a common limitation of motion-only tracking systems, since no appearance information is used to distinguish similar vehicles.\n",
    "\n",
    "#### 3. Missed Detections\n",
    "\n",
    "The system sometimes failed to detect vehicles due to:\n",
    "\n",
    "- Strong glare regions in the road.\n",
    "\n",
    "- Sudden illumination changes.\n",
    "\n",
    "- Imperfect segmentation.\n",
    "\n",
    "- Vehicles blending with the background.\n",
    "\n",
    "When detections are missed:\n",
    "\n",
    "- The Kalman filter predicts the position.\n",
    "\n",
    "- If the object reappears within MAX_AGE, the identity is preserved.\n",
    "\n",
    "- Otherwise, a new ID may be created.\n",
    "\n",
    "There is a trade-off between:\n",
    "\n",
    "- Increasing MAX_AGE → better occlusion tolerance but risk of keeping dead tracks.\n",
    "\n",
    "- Decreasing MAX_AGE → cleaner tracking but more ID fragmentation.\n",
    "\n",
    "#### 4. Strong Perspective Dependency\n",
    "\n",
    "The area filtering model is tuned specifically to the camera perspective of this video.\n",
    "\n",
    "The expected vehicle size is computed as a function of vertical image position.\n",
    "\n",
    "While this improves detection quality in this scene, it introduces a limitation:\n",
    "\n",
    "- The system would not generalize well to a different camera angle.\n",
    "\n",
    "- The area model would need to be re-estimated.\n",
    "\n",
    "This makes the detector scene-dependent.\n",
    "\n",
    "#### 5. Lighting Artifacts\n",
    "\n",
    "In this particular video, a bright region on the road caused frequent tracking failures.\n",
    "\n",
    "High-intensity areas affected:\n",
    "\n",
    "- Foreground segmentation quality.\n",
    "\n",
    "- Blob consistency.\n",
    "\n",
    "- Stability of centroids.\n",
    "\n",
    "Background subtraction methods are inherently sensitive to lighting variations and reflections.\n",
    "\n",
    "#### 6. Vehicle Color Similarity\n",
    "\n",
    "Vehicles with colors similar to the road surface were more difficult to segment.\n",
    "\n",
    "For example:\n",
    "\n",
    "- A car whose color blended with lane markings was poorly detected in early frames.\n",
    "\n",
    "- This caused unstable tracking initialization.\n",
    "\n",
    "Since MOG2 relies on pixel intensity differences, low contrast objects are harder to detect reliably.\n",
    "\n",
    "#### 7. MOG2 Warm-Up Effect\n",
    "\n",
    "The MOG2 background subtractor learns the background model online.\n",
    "\n",
    "At the beginning of the video:\n",
    "\n",
    "- The model is not yet stable.\n",
    "\n",
    "- Moving vehicles may be partially absorbed into the background model.\n",
    "\n",
    "- Foreground masks are inconsistent.\n",
    "\n",
    "Because the video is relatively short, this stabilization period has a noticeable impact on early detections.\n",
    "\n",
    "### Possible Improvements\n",
    "\n",
    "Several modifications could improve robustness and generalization.\n",
    "\n",
    "#### 1. Use an Appearance Model\n",
    "\n",
    "Currently, tracking relies only on motion information.\n",
    "\n",
    "Adding appearance features (such as color histograms or embeddings) could:\n",
    "\n",
    "- Reduce ID switching.\n",
    "\n",
    "- Improve discrimination between nearby vehicles.\n",
    "\n",
    "- Increase robustness in crowded scenarios.\n",
    "\n",
    "#### 2. Use Hungarian Algorithm for Optimal Assignment\n",
    "\n",
    "The current association method uses greedy nearest-neighbor matching.\n",
    "\n",
    "Replacing it with the Hungarian algorithm would:\n",
    "\n",
    "- Provide globally optimal matching.\n",
    "\n",
    "- Reduce ambiguous assignments.\n",
    "\n",
    "- Improve stability when multiple vehicles are close together.\n",
    "\n",
    "#### 3. Replace Background Subtraction with a Deep Detector\n",
    "\n",
    "Instead of relying on MOG2, a deep learning detector (e.g., YOLO) could be used.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- More robust to lighting variations.\n",
    "\n",
    "- Less sensitive to shadows and glare.\n",
    "\n",
    "- Better generalization across scenes.\n",
    "\n",
    "This would significantly improve detection reliability.\n",
    "\n",
    "#### 4. Shadow Removal Techniques\n",
    "\n",
    "Explicit shadow removal could reduce false positives and blob fragmentation.\n",
    "\n",
    "Techniques such as:\n",
    "\n",
    "- HSV-based shadow filtering\n",
    "\n",
    "- Illumination-invariant representations\n",
    "\n",
    "could improve mask quality.\n",
    "\n",
    "#### 5. Adaptive Parameter Tuning\n",
    "\n",
    "Instead of fixed thresholds:\n",
    "\n",
    "- The gating distance could adapt dynamically based on motion variance.\n",
    "\n",
    "- MAX_AGE could adjust based on object speed.\n",
    "\n",
    "- Noise parameters could be updated online.\n",
    "\n",
    "This would make the system more flexible in varying conditions.\n",
    "\n",
    "#### 6. Re-Identification Module\n",
    "\n",
    "For long occlusions, a re-identification mechanism could be added.\n",
    "\n",
    "This would allow:\n",
    "\n",
    "- Recovering identities after long disappearances.\n",
    "\n",
    "- Maintaining consistency even across partial scene exits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef13b8e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sub11761",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
